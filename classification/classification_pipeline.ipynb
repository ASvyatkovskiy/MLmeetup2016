{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection and preprocessing\n",
    "\n",
    "We will run the preprocessing step with a small fraction of data. Test with the full dataset will be illustrated during the live demo on the cluster. \n",
    "\n",
    "## File descriptions\n",
    "\n",
    "{0,1,2,3,4}.zip - are all HTML files. Files listed in train_v2.csv are training files:\n",
    "\n",
    "file - the raw file name\n",
    "labels: 0 - organic content; 1 - sponsored content\n",
    "\n",
    "Following is the avro schema (contents of the avsc file) for the labels dataset:\n",
    "\n",
    "```bash\n",
    "{\"namespace\": \"html.avro\",\t\t\n",
    "\t \"type\": \"record\",\t\n",
    "\t \"name\": \"Label\",\t\n",
    "\t \"fields\": [\t\n",
    "\t     {\"name\": \"id\", \"type\": \"string\"},\t\n",
    "\t     {\"name\": \"label\", \"type\": \"double\"}\t\n",
    "\t ]\t\n",
    "\t}\t\n",
    "```\n",
    "\n",
    "Labels dataset needs tno preprocessing.\n",
    "\n",
    "The corresponding documents (HTML web pages) are in the zip files:  \n",
    "\n",
    "```bash\n",
    "0,1,2,3,4.zip\n",
    "```\n",
    "\n",
    "And will have a following structure after preprocessing:\n",
    "\n",
    "```bash\n",
    "{\"namespace\": \"html.avro\",\n",
    " \"type\": \"record\",\n",
    " \"name\": \"Html\",\n",
    " \"fields\": [\n",
    "     {\"name\": \"id\", \"type\": \"string\"},\n",
    "     {\"name\": \"images\",  \"type\": {\"type\":\"array\", \"items\":\"string\"}},\n",
    "     {\"name\": \"links\", \"type\": {\"type\":\"array\", \"items\":\"string\"}},\n",
    "     {\"name\": \"text\", \"type\": \"string\"},\n",
    "     {\"name\": \"title\", \"type\": {\"type\":\"array\", \"items\":\"string\"}}\n",
    " ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Web scraping\n",
    "\n",
    "The term \"scraping\" refers to getting unstructured data and turning it into something usable in the analysis (or making it structured or semi-structured data). The tools available through Python are mature and easy to use. In our case the source data comes in HTML form. Typically we would use urllib to download pages on the web, but they are already downloaded for us here.\n",
    "\n",
    "### The basic workflow is:\n",
    "\n",
    "Find the data you want on the web *(again, in our case it is provided to us)*.\n",
    "\n",
    "Inspect the page you are dealing with, to figure out how to zoom-in towards the content you want. This will involve some combiation of looking at the source code of the page (especially if it is simple), and\n",
    "figuring out the structure of the HTML parse tree. This step is much easier with a something like *Chrome Developer Tools*.\n",
    "\n",
    "Write code to get out what you want:\n",
    "If the page is very simple, treat it as a bunch of text => string manipulation / regular expressions in Python.\n",
    "If the page is more complicated (and/or written in good style), we want to use the HTML parse tree => BeautifulSoup in Python.\n",
    "\n",
    "### CSS selectors\n",
    "\n",
    "For most of the real world HTML documents, relular experession will not work well, as one has to use a lot of nested finds for specific classes/tags. It's so common that there is a special convenience language for such traversals: CSS selectors. BeautifulSoup supports a form of CSS selectors, and this will let us write the above in a more concise and expressive way:\n",
    "\n",
    "> Some basic building examples of selectors are:\n",
    "'mytag' picks out all tags of type mytag.\n",
    "'#myid' picks out all tags whose id is equal to myid\n",
    "'.myclass' picks out all tags whose class is equal to myclass\n",
    "'mytag#myid' will pick all tags of type mytag and id equal to myid (analgously for 'mytag.myclass')\n",
    "If 'selector1' and 'selector2' are two selectors, then there is another selector 'selector1 selector2'. It picks out all tags satisfying selector2 that are descendents(*) of something satisfying selector1, i.e., it's like our nested find.\n",
    "(*) It doesn't have to be a direct descedent. I.e., it can be a grand-grand-..-grand-child of something satisfying selector1. For direct descendents we'd instead write 'selector1 > selector2'\n",
    "\n",
    "## Example with our data\n",
    "\n",
    "First, make sure that your notebook is running the Python from your Anaconda environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.11 |Continuum Analytics, Inc.| (default, Dec  6 2015, 18:57:58) \\n[GCC 4.2.1 (Apple Inc. build 5577)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexeys/anaconda/envs/meetup_env/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "document = open(\"../preprocess/data/1118089_raw_html.txt\",\"r\").read()\n",
    "soup = BeautifulSoup(document)\n",
    "print soup.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find specific tags, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"site-title\" itemprop=\"headline\">\\n<a href=\"http://www.almostsupermom.com/\">\\n        Almost Supermom\\n       </a>\\n</p>]\n"
     ]
    }
   ],
   "source": [
    "elements = soup.find_all('p', attrs={'class':'site-title'})\n",
    "print elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser we are going to use to extract text from our HTML pages consists of 2 find statements: one that looks for div tags of class \"text\" and the other one, more relaxed, that looks for all tags p:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(soup):\n",
    "    \"\"\" parameters:\n",
    "            - soup: beautifulSoup4 parsed html page\n",
    "        out:\n",
    "            - textdata: a list of parsed text output by looping over html paragraph tags\n",
    "        note:\n",
    "            - could soup.get_text() instead but the output is more noisy \"\"\"\n",
    "    textdata = ['']\n",
    "\n",
    "    #this is more strict \n",
    "    for tag in soup.find_all(\"div\", {\"class\":\"text\"}):\n",
    "        try:\n",
    "           textdata.append(tag.text.encode('ascii','ignore').strip())\n",
    "        except Exception:\n",
    "           continue\n",
    " \n",
    "    #more relaxed\n",
    "    for text in soup.find_all('p'):\n",
    "        try:\n",
    "            textdata.append(text.text.encode('ascii','ignore').strip())\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Almost Supermom',\n",
       " 'Able to Leap Piles of Laundry in a Single Bound',\n",
       " 'July 13, 2015\\n         \\n         by\\n         \\n\\n\\n            Jordyn\\n           \\n\\n\\n\\n\\n           Leave a Comment',\n",
       " 'Necessity is the mother of invention, so is the case with this scrumptious\\n         \\n          gluten free greek flatbread.',\n",
       " 'It is so hot here in Atlanta. We are practically melting in the oppressive, sticky heat that the south is known for. Its the kind of heat that makes you want to sit on the couch and do nothing until the sun goes down. Turning on the oven is a repulsive thought as it will contribute to the overwhelming heat, so we nourish ourselves with salads and cold sandwiches in attempt to avoid turning into a puddle.',\n",
       " 'The problem with salads and sandwiches is they can get boring. Really boring. In attempt to shake things up but stay cool, Ive been experimenting with different recipes that taste delicious, but require no cooking , at least not in the heat of the day.',\n",
       " '',\n",
       " 'One of the recipes that I created is this\\n         \\n          Gluten Free Greek Flatbread,\\n         \\n         I can make the crust the night before when it isnt so hot and then throw the toppings on the next day for a quick and easy gluten free lunch.',\n",
       " '',\n",
       " '',\n",
       " 'Ingredients',\n",
       " 'Instructions',\n",
       " '',\n",
       " 'You definitely have to try this gluten free greek flatbread, it has become one of my favorite lunches and beats the heck out of a boring sandwich or salad any day. Enjoy!',\n",
       " '',\n",
       " 'Filed Under:\\n          \\n           Gluten Free\\n          \\n          ,\\n          \\n           Recipes',\n",
       " 'Your email address will not be published.\\n         \\n         Required fields are marked\\n         \\n          *',\n",
       " 'Name\\n          \\n           *',\n",
       " 'Email\\n          \\n           *',\n",
       " 'Website',\n",
       " 'Comment',\n",
       " 'You may use these\\n         \\n          HTML\\n         \\n         tags and attributes:\\n         \\n          <a href=\"\" title=\"\"> <abbr title=\"\"> <acronym title=\"\"> <b> <blockquote cite=\"\"> <cite> <code> <del datetime=\"\"> <em> <i> <q cite=\"\"> <strike> <strong>',\n",
       " '',\n",
       " '',\n",
       " 'Sign me up for the newsletter!',\n",
       " '',\n",
       " 'Return to top',\n",
       " 'Site Design by:\\n       \\n        Fantastique Designs\\n       \\n       \\r\\nCopyright  2014']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_text(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cleaning step\n",
    "\n",
    "As you can see, the text we have extracted contains a lot of spurios white spaces and non-alphanumeric characters. Also, it comes as a list of strings. We are going to concatenate it all in 1 long string, remove those characters, and substitute multiple white spaces with just a single one:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text_as_list):\n",
    "    text_as_string = \" \".join(text_as_list)\n",
    "    text_as_string = re.sub('\\s+',' ',text_as_string)\n",
    "    text_as_string = text_as_string.encode(\"utf8\").translate(None,'=@&$/%?<>,[]{}()*.0123456789:;-\\'\"_').lower()\n",
    "\n",
    "    return text_as_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "almost supermom able to leap piles of laundry in a single bound july   by jordyn leave a comment necessity is the mother of invention so is the case with this scrumptious gluten free greek flatbread it is so hot here in atlanta we are practically melting in the oppressive sticky heat that the south is known for its the kind of heat that makes you want to sit on the couch and do nothing until the sun goes down turning on the oven is a repulsive thought as it will contribute to the overwhelming heat so we nourish ourselves with salads and cold sandwiches in attempt to avoid turning into a puddle the problem with salads and sandwiches is they can get boring really boring in attempt to shake things up but stay cool ive been experimenting with different recipes that taste delicious but require no cooking  at least not in the heat of the day one of the recipes that i created is this gluten free greek flatbread i can make the crust the night before when it isnt so hot and then throw the toppings on the next day for a quick and easy gluten free lunch ingredients instructions you definitely have to try this gluten free greek flatbread it has become one of my favorite lunches and beats the heck out of a boring sandwich or salad any day enjoy! filed under gluten free  recipes your email address will not be published required fields are marked  name  email  website comment you may use these html tags and attributes a href title abbr title acronym title b blockquote cite cite code del datetime em i q cite strike strong sign me up for the newsletter! return to top site design by fantastique designs copyright \n"
     ]
    }
   ],
   "source": [
    "textdata = filter(None,parse_text(soup))\n",
    "print clean_text(textdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We skip the preprocessing step for the sake of saving time. You are encouraged to complete this step offline to obtain the JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting JSON to Avro\n",
    "\n",
    "Once the JSON files are obtained, use avro-tools.jar\n",
    "\n",
    "```bash\n",
    "java -jar avro-tools-1.7.7.jar fromjson --schema-file labels.avsc labels.json > labels.avro\n",
    "java -jar avro-tools-1.7.7.jar fromjson --schema-file html.avsc html.json > html.avro\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and exploring preprocessed data \n",
    "\n",
    "Download the preprocessed samples from Dropbox (using, for instance, wget):\n",
    "\n",
    "labels: https://www.dropbox.com/s/lhi8kbgtharwn2x/labels.avro?dl=0\n",
    "data: https://www.dropbox.com/s/5f1zy74o4igxsgd/html.avro?dl=0\n",
    "\n",
    "Then, place those avro files to data folder:\n",
    "\n",
    "```bash\n",
    "mkdir classification/data\n",
    "mv labels.avro html.avro classification/data\n",
    "```\n",
    "\n",
    "## Databricks spark-avro\n",
    "\n",
    "Apache Avro is a data serialization system. We have added the dependency on spark-avro to link against the Databricks spark-avro library in the pom file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingest data...\n",
      "root\n",
      " |-- id: string (nullable = false)\n",
      " |-- images: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- links: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- text: string (nullable = false)\n",
      " |-- title: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = false)\n",
      " |-- label: double (nullable = false)\n",
      "\n",
      "+-------+--------------------+\n",
      "|     id|                text|\n",
      "+-------+--------------------+\n",
      "|1309896|insolite art et a...|\n",
      "|1309926|israeli missile n...|\n",
      "|1309968|the page you are ...|\n",
      "|1310010|advertisement #bd...|\n",
      "|1310016|make more money !...|\n",
      "|1310058| view comments lo...|\n",
      "|1310064|posted in windows...|\n",
      "|1310106|voor veel mensen ...|\n",
      "|1310178|whats holding you...|\n",
      "|1310214|get the best free...|\n",
      "|1310226|v e n m s u l t a...|\n",
      "|1310256|monthly mixes for...|\n",
      "|1310346|youre helping us ...|\n",
      "|1310352|                    |\n",
      "|1310382|visitscotland use...|\n",
      "|1310448|share pin tweet s...|\n",
      "|1310520|advertisement for...|\n",
      "|1310538|suds is a lightwe...|\n",
      "|1310562|t m l l g e n t r...|\n",
      "|1310580|a critical crossr...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Ingest data...\"\n",
    "\n",
    "train_label_df = sqlContext.read.format('com.databricks.spark.avro').load(\"./data/labels.avro\")\n",
    "input_df = sqlContext.read.format('com.databricks.spark.avro').load(\"./data/html.avro\")\n",
    "input_df.printSchema()\n",
    "train_label_df.printSchema()\n",
    "input_df.select(\"id\",\"text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'1310214', text=u'get the best free newsletter money can buy source wikipediaorg source tsunemi kubodera of the national science museum of japanap source wikipediaorg source wikipediaorg source crchicagoweeblycom source wikipediaorg source wikipediaorg source thecatalinaislandercom source wikipediaorg source wikipediaorg source wikipediaorg source wikipediaorg source bodegaheadblogspotcom source wikipediaorg source joi source animalwildlifeblogspotcom step away from the pesticide no girls allowed! clueless or lack of compassion were totally geeking out over here put your mindbody connection to the test')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good example\n",
    "input_df.select(\"id\",\"text\").where(input_df.id == 1310214).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'1310562', text=u't m l l g e n t r u e f o r h t t p w w w p i c t u r e s o f e n g l a n d c o m r c b l z n z o z v z m e t a n a m e m s s m a r t t a g s p r e v e n t p a r s i n g c o n t e n t t r u e m e t a n a m e a u t h o r c o n t e n t j a m e s w i l s o n l i n k r e l m e t a h r e f h t t p w w w p i c t u r e s o f e n g l a n d c o m l a b e l s x m l t y p e a p p l i c a t i o n r d f + x m l t i t l e i c r a l a b e l s l i n k r e l s t y l e s h e e t t y p e t e x t c s s m e d i a s c r e e n h r e f c s s s t y l e n e w c s s l i n k r e l s t y l e s h e e t t y p e t e x t c s s m e d i a p r i n t h r e f c s s p r i n t c s s l i n k r e l s t y l e s h e e t t y p e t e x t c s s m e d i a s c r e e n h r e f c s s t o o l t i p s t e r c s s l i n k r e l s t y l e s h e e t t y p e t e x t c s s m e d i a s c r e e n h r e f c s s t o o l t i p s t e r n o i r c s s l i n k r e l s t y l e s h e e t t y p e t e x t c s s m e d i a s c r e e n h r e f s l i c k s l i c k c s s l i n k r e l s t y l e s h e e t t y p e t e x t c s s h r e f e n g i n e s t y l e c s s m e t a n a m e g o o g l e s i t e v e r i f i c a t i o n c o n t e n t m r t w b s s d g h f n w u c x z m f t t f g b s z y u u l o i g q ! l i n k r e l s t y l e s h e e t t y p e t e x t c s s m e d i a s c r e e n h r e f c s s s o u t h s t r e e t j q u e r y u i m i n c s s s c r i p t t y p e t e x t j a v a s c r i p t s r c j s j q u e r y u i m i n j s s c r i p t l i n k r e l s t y l e s h e e t t y p e t e x t c s s m e d i a s c r e e n h r e f c s s l a t e r o o m s c s s l i n k r e l s t y l e s h e e t t y p e t e x t c s s m e d i a s c r e e n h r e f c s s b o o k i n g d o t c o m c s s s c r i p t l a n g u a g e j a v a s c r i p t t y p e t e x t j a v a s c r i p t s r c j s b o o k i n g d o t c o m j s s c r i p t s c r i p t t y p e t e x t j a v a s c r i p t s r c j s j q u e r y m i n j s s c r i p t s c r i p t l a n g u a g e j a v a s c r i p t t y p e t e x t j a v a s c r i p t s r c j s j q u e r y t o o l t i p s t e r m i n j s s c r i p t s c r i p t l a n g u a g e j a v a s c r i p t t y p e t e x t j a v a s c r i p t s r c j s j a v a s c r i p t n e w j s s c r i p t s c r i p t l a n g u a g e j a v a s c r i p t t y p e t e x t j a v a s c r i p t s r c j s f s d a t e s e l e c t j s s c r i p t ! s c r i p t l a n g u a g e j a v a s c r i p t t y p e t e x t j a v a s c r i p t s r c j s h o m e p a g e s l i d e s h o w j s s c r i p t s c r i p t t y p e t e x t j a v a s c r i p t v a r g a q g a q | | g a q p u s h s e t a c c o u n t u a g a q p u s h t r a c k p a g e v i e w f u n c t i o n v a r g a d o c u m e n t c r e a t e e l e m e n t s c r i p t g a t y p e t e x t j a v a s c r i p t g a a s y n c t r u e g a s r c h t t p s d o c u m e n t l o c a t i o n p r o t o c o l h t t p s s s l h t t p w w w + g o o g l e a n a l y t i c s c o m g a j s v a r s d o c u m e n t g e t e l e m e n t s b y t a g n a m e s c r i p t s p a r e n t n o d e i n s e r t b e f o r e g a s s c r i p t s c r i p t l a n g u a g e j a v a s c r i p t ! i d e a b y n i c w o l f e ! t h i s s c r i p t a n d m a n y m o r e a r e a v a i l a b l e f r e e o n l i n e a t ! t h e j a v a s c r i p t s o u r c e ! ! h t t p j a v a s c r i p t i n t e r n e t c o m ! b e g i n f u n c t i o n p o p u p u r l d a y n e w d a t e i d d a y g e t t i m e e v a l p a g e + i d + w i n d o w o p e n u r l + i d + t o o l b a r s c r o l l b a r s l o c a t i o n s t a t u s b a r m e n u b a r r e s i z a b l e w i d t h h e i g h t l e f t t o p e n d s c r i p t ! p l a c e t h i s r e n d e r c a l l w h e r e a p p r o p r i a t e s c r i p t t y p e t e x t j a v a s c r i p t f u n c t i o n v a r p o d o c u m e n t c r e a t e e l e m e n t s c r i p t p o t y p e t e x t j a v a s c r i p t p o a s y n c t r u e p o s r c h t t p s a p i s g o o g l e c o m j s p l u s o n e j s v a r s d o c u m e n t g e t e l e m e n t s b y t a g n a m e s c r i p t s p a r e n t n o d e i n s e r t b e f o r e p o s s c r i p t s c r i p t f u n c t i o n d o c u m e n t r e a d y f u n c t i o n t o o l t i p t o o l t i p s t e r \\t \\t \\t s p e e d \\t \\t j q u e r y s c r i p t s c r i p t \\t j q u e r y n o c o n f l i c t \\t j q u e r y d o c u m e n t r e a d y f u n c t i o n j q u e r y i m g w r a p p e r h o v e r f u n c t i o n j q u e r y t h i s c h i l d r e n i m g n a v s t o p f a d e t o f u n c t i o n j q u e r y t h i s c h i l d r e n i m g n a v s t o p f a d e t o s c r i p t s c r i p t j q u e r y n o c o n f l i c t j q u e r y d o c u m e n t r e a d y f u n c t i o n j q u e r y r e q h i d e s c r i p t \\t \\t ! j a v a s c r i p t t o c l e a r s e a r c h t e x t w h e n t h e f i e l d i s c l i c k e d s c r i p t t y p e t e x t j a v a s c r i p t w i n d o w o n l o a d f u n c t i o n \\t g e t s u b m i t b u t t o n \\t v a r s u b m i t b u t t o n d o c u m e n t g e t e l e m e n t b y i d t f q \\t a d d l i s t e n e r t o s u b m i t b u t t o n \\t i f s u b m i t b u t t o n a d d e v e n t l i s t e n e r \\t \\t s u b m i t b u t t o n a d d e v e n t l i s t e n e r c l i c k f u n c t i o n \\t \\t \\t i f s u b m i t b u t t o n v a l u e s e a r c h o u r w e b s i t e c u s t o m i z e t h i s t e x t s t r i n g t o w h a t e v e r y o u w a n t \\t \\t \\t \\t s u b m i t b u t t o n v a l u e \\t \\t \\t \\t \\t \\t s c r i p t ! s c r i p t j q u e r y n o c o n f l i c t j q u e r y d o c u m e n t r e a d y f u n c t i o n j q u e r y # r a d i o b u t t o n s e t s c r i p t h e a d b o d y x m l l a n g e n a n a m e t o p a d i v i d o u t e r c o n t a i n e r d i v i d i n n e r c o n t a i n e r d i v i d b o r d e r d i v i d b o r d e r d i v i d b o r d e r d i v i d b o r d e r d i v i d h e a d e r o u t e r d i v i d h e a d e r i n n e r d i v i d h e a d e r t o p o v e r p h o t o s o f e n g l a n d b r + m e m b e r s b r a h r e f u s e r r e g i s t e r c l i c k h e r e t o j o i n t h e c o m m u n i t y a d i v a h r e f i m g s r c i m a g e s l o g o j p g c l a s s l o g o i m g b o r d e r a l t p i c t u r e s o f e n g l a n d a l i g n l e f t a a h r e f i m g s r c i m a g e s l o g o g i f c l a s s r e s p o n s i v e i m a g e b o r d e r a l t a l i g n b o t t o m a b r n b s p h c l a s s h e a d e r s t r a p e x p l o r i n g t h e m o s t p i c t u r e s q u e a m p h i s t o r i c p a r t s o f e n g l a n d h d i v d i v d i v i d n a v i g a t i o n d i v c l a s s m a i n s e a r c h c o n t a i n e r f o r m a c t i o n g s r e s u l t s m e t h o d g e t i d m y f o r m i n p u t t y p e t e x t n a m e q i d u s e r i n p u t i n p u t t y p e h i d d e n n a m e c x v a l u e p a r t n e r p u b i d g o n e i n p u t t y p e h i d d e n n a m e c o f v a l u e f o r i d i d g t w o i n p u t t y p e h i d d e n n a m e i e v a l u e u t f i d g t h r e e \\t \\t \\t \\t \\t \\t \\t \\t s e l e c t o n c h a n g e t h i s f o r m a c t i o n t h i s v a l u e i d s i t e n a m e s i t e \\t \\t o p t i o n v a l u e g s r e s u l t s s e l e c t e d s e l e c t e d s i t e o p t i o n o p t i o n v a l u e i m a g e s e a r c h q q i m a g e s o n l y o p t i o n s e l e c t \\t i n p u t t y p e s u b m i t n a m e s a v a l u e s e a r c h f o r m s c r i p t j q u e r y n o c o n f l i c t j q u e r y # s i t e o n c h a n g e f u n c t i o n \\t j q u e r y # g o n e a t t r v a l u e \\t j q u e r y # g t w o a t t r v a l u e \\t j q u e r y # g t h r e e a t t r v a l u e \\t s c r i p t d i v d i v t a b l e b o r d e r c e l l p a d d i n g c e l l s p a c i n g w i d t h t r t d v a l i g n t o p w i d t h d i v i d l h s ! h s i t e s e a r c h h d i v c l a s s b o x s f o r m a c t i o n h t t p w w w p i c t u r e s o f e n g l a n d c o m g s r e s u l t s i d c s e s e a r c h b o x c l a s s s e a r c h m a i n i n p u t t y p e h i d d e n n a m e c x v a l u e p a r t n e r p u b i n p u t t y p e h i d d e n n a m e c o f v a l u e f o r i d i n p u t t y p e h i d d e n n a m e i e v a l u e u t f i n p u t t y p e t e x t n a m e q c l a s s m a i n s e a r c h i n p u t t y p e s u b m i t n a m e s a v a l u e g o c l a s s m a i n s e a r c h b u t t o n f o r m \\t s c r i p t t y p e t e x t j a v a s c r i p t s r c h t t p w w w g o o g l e c o m c o o p c s e b r a n d f o r m c s e s e a r c h b o x a m p l a n g s c r i p t d i v h n b s p h d i v c l a s s m a i n m e n u p a h r e f h o m e a | a h r e f u s e r r e g i s t e r j o i n a | a h r e f u s e r l o g i n l o g i n a b r a h r e f s u b m i t p i c t u r e s a d d p i c t u r e s a b r a h r e f l a t e s t p i c t u r e s l a t e s t p i c t u r e s a b r a h r e f l a t e s t c o m m e n t s l a t e s t c o m m e n t s a b r a h r e f f o r u m s l a t e s t l a t e s t f o r u m s a b r a h r e f f a v o u r i t e p i c t u r e s r e c e n t r e c e n t f a v o u r i t e s a b r a h r e f l a t e s t t o u r s m e m b e r t o u r s a b r a h r e f h o t e l s e n g l a n d h o t e l s a b r a h r e f c o t s w o l d s t h e c o t s w o l d s a b r a h r e f e n g l a n d c u m b r i a t h e l a k e d i s t r i c t l a k e d i s t r i c t a b r a h r e f c o u n t r y s i d e e n g l i s h c o u n t r y s i d e a b r a h r e f e n g l a n d t o w n s h i s t o r i c c i t i e s h i s t o r i c c i t i e s a b r a h r e f e n g l a n d t o w n s h i s t o r i c m a r k e t t o w n s m a r k e t t o w n s a b r a h r e f e n g l a n d t o w n s p i c t u r e s q u e v i l l a g e s p i c t u r e s q u e v i l l a g e s a b r p d i v h b e x p l o r e e n g l a n d b h d i v c l a s s b o x l p a h r e f e n g l a n d c o u n t i e s c o u n t i e s a b r a h r e f e n g l a n d t o w n s t o w n s a b r a h r e f e n g l a n d a t t r a c t i o n s a t t r a c t i o n s a b r a h r e f m a p o f e n g l a n d e n g l a n d m a p s a p p b p i c t u r e t o u r s b b r a h r e f e n g l a n d t o u r e n g l a n d i n w i n t e r e n g l a n d i n w i n t e r a b r a h r e f e n g l a n d t o u r e n g l i s h c o t t a g e s e n g l i s h c o t t a g e s a b r a h r e f e n g l a n d t o u r c h u r c h e s c h u r c h e s a b r a h r e f c o u n t r y s i d e e n g l i s h c o u n t r y s i d e a b r a h r e f e n g l a n d t o u r e n g l i s h c o a s t a l s c e n e s e n g l i s h c o a s t a l s c e n e s a b r a h r e f e n g l a n d t o u r v i l l a g e s c e n e s v i l l a g e s c e n e s a b r a h r e f e n g l a n d t o u r s m o r e a p p b a t t r a c t i o n s b b r a h r e f c o t s w o l d s t h e c o t s w o l d s a b r a h r e f e n g l a n d c u m b r i a t h e l a k e d i s t r i c t t h e l a k e d i s t r i c t a b r a h r e f n a t i o n a l p a r k s n a t i o n a l p a r k s a b r a h r e f e n g l a n d a t t r a c t i o n s a b b e y s a b b e y s a b r a h r e f e n g l a n d a t t r a c t i o n s b e a u t y s p o t s b e a u t y s p o t s a b r a h r e f e n g l a n d a t t r a c t i o n s c a s t l e s c a s t l e s a b r a h r e f e n g l a n d a t t r a c t i o n s s t a t e l y h o m e s s t a t e l y h o m e s a b r a h r e f e n g l a n d a t t r a c t i o n s n a t i o n a l t r u s t a t t r a c t i o n s n a t i o n a l t r u s t a b r a h r e f e n g l a n d a t t r a c t i o n s t y p e m o r e c a t e g o r i e s a p p b m o r e b b r a h r e f a d v e r t i s e a d v e r t i s e a b r a h r e f p o e m s e n g l a n d p o e m s a b r a h r e f e n g l a n d a r t i c l e s e n g l a n d a r t i c l e s a b r a h r e f s t o c k p h o t o s e n g l a n d s t o c k p h o t o s a b r a h r e f e n g l a n d e v e n t s e v e n t s i n e n g l a n d a b r a h r e f m e m b e r s p r e m i e r m e m b e r s d i r e c t o r y a b r a h r e f t r i p p l a n n i n g t r i p p l a n n i n g a b r a h r e f e n g l a n d f a c t s e n g l a n d f a c t s a b r a h r e f t r a d i t i o n a l e n g l i s h r e c i p e s t r a d i t i o n a l e n g l i s h r e c i p e s a b r a h r e f h i s t o r y h i s t o r y a b r a h r e f e n g l a n d f l a g s e n g l a n d f l a g s a b r a h r e f a b o u t a b o u t a b r a h r e f c o n t a c t c o n t a c t a b r a h r e f g u e s t b o o k g u e s t b o o k a b r a h r e f d o n a t e d o n a t e a b r p d i v d i v t d t d v a l i g n t o p w i d t h d i v i d m a i n d i v i d c o n t e n t o u t e r d i v i d c o n t e n t i n n e r d i v c l a s s t r a i l a h r e f e n g l a n d e n g l a n d a g t a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r s h i r e a d i v h p i c t u r e s o f l e i c e s t e r s h i r e h t a b l e b o r d e r c e l l p a d d i n g c e l l s p a c i n g a l i g n c e n t e r t r t d i m g a l t b r a d g a t e p a r k n e a r l e i c e s t e r o l d j o h n t o w e r c l a s s i m a g e s r c i m g m j p g w i d t h h e i g h t b o r d e r t d t d u l l i s t r o n g a h r e f e n g l a n d l e i c e s t e r s h i r e p i c t u r e s l e i c e s t e r s h i r e p i c t u r e t o u r a s t r o n g l i s t r o n g a h r e f e n g l a n d l e i c e s t e r s h i r e h o t e l s l e i c e s t e r s h i r e h o t e l s a s t r o n g l i s t r o n g a h r e f e n g l a n d l e i c e s t e r s h i r e f a c t s l e i c e s t e r s h i r e f a c t s a s t r o n g l i a h r e f # a b o u t a b o u t l e i c e s t e r s h i r e a l i a h r e f # r e c o m m e n d e d t o w n s r e c o m m e n d e d t o w n s a l i a h r e f # r e c o m m e n d e d a t t r a c t i o n s r e c o m m e n d e d a t t r a c t i o n s a l i a h r e f e n g l a n d l e i c e s t e r s h i r e m a p l e i c e s t e r s h i r e m a p a l i a h r e f e n g l a n d l e i c e s t e r s h i r e p i c t u r e s a d d u p l o a d p i c t u r e a u l t d t r t a b l e p a l i g n c e n t e r s t r o n g a h r e f e n g l a n d l e i c e s t e r s h i r e s t o c k p h o t o s r o y a l t y f r e e s t o c k p h o t o s o f l e i c e s t e r s h i r e a s t r o n g p a n a m e a b o u t a h a b o u t l e i c e s t e r s h i r e h d i v \\t \\t t h e c o u n t y o f l e i c e s t e r s h i r e i s s i t u a t e d i n t h e h e a r t o f e n g l a n d t h e r e a r e m a n y p l a c e s o f i n t e r e s t t o v i s i t i n c l u d i n g t h e c i t y o f l e i c e s t e r n o t a b l e a t t r a c t i o n s i n c l u d e l e i c e s t e r c a t h e d r a l t h e n a t i o n a l s p a c e c e n t r e t h e u k s l a r g e s t a t t r a c t i o n d e d i c a t e d t o s p a c e s c i e n c e a n d a s t r o n o m y n e w w a l k m u s e u m a n d a r t g a l l e r y s i t u a t e d w i t h i n t h e h i s t o r i c n e w w a l k a r e a o f t h e c i t y a n d b e l g r a v e h a l l b u i l t i n t h e e a r l y t h c e n t u r y i n w h a t w a s t h e n a s m a l l v i l l a g e m i l e s f r o m t h e t o w n o f l e i c e s t e r l e i c e s t e r s h i r e b o a s t s s o m e o f e n g l a n d s m o s t b e a u t i f u l c o u n t r y s i d e a n d i n t e r e s t i n g t o w n s a n d v i l l a g e s d i v a h r e f f o r u m a d d c o n t e n t a d d u p d a t e d e s c r i p t i o n a b r b r s e e b e l o w f o r a l l t h e b e s t t o w n s a t t r a c t i o n s i n l e i c e s t e r s h i r e o r s e e a s e l e c t i o n o f o u r l e i c e s t e r s h i r e p h o t o g r a p h s o n o u r a h r e f e n g l a n d l e i c e s t e r s h i r e p i c t u r e s b p i c t u r e t o u r o f l e i c e s t e r s h i r e b a b r b r b a h r e f e n g l a n d l e i c e s t e r s h i r e h o t e l s h o t e l s i n l e i c e s t e r s h i r e a b f i n d d i s c o u n t l e i c e s t e r s h i r e h o t e l s a n d l e i c e s t e r s h i r e a c c o m m o d a t i o n b r a n a m e r e c o m m e n d e d t o w n s a h r e c o m m e n d e d t o w n s v i l l a g e s i n l e i c e s t e r s h i r e h t a b l e b o r d e r c e l l p a d d i n g c e l l s p a c i n g w i d t h t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e a n s t e y i m g a l t g r o b y r o a d a n s t e y l e i c e s t e r c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e a n s t e y a n s t e y a h p c l a s s s m a l l o n e o f t h e m o s t a t t r a c t i v e f e a t u r e s o f a n s t e y i s t h e f a m o u s t h c e n t u r y f i v e a r c h e d p a c k h o r s e b r i d g e c r o s s i n g t h e r i v e r w r e a k e p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e a n s t e y i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e a n s t e y p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e a n s t e y h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e a s h b y d e l a z o u c h i m g a l t t h e c a t h o l i c c h u r c h o f o u r l a d y o f l o u r d e s a s h b y d e l a z o u c h l e i c e s t e r s h i r e c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e a s h b y d e l a z o u c h a s h b y d e l a z o u c h a h p c l a s s s m a l l t h i s t o w n t a k e s i t s n a m e f r o m t h e l a s o u c h e f a m i l y l o r d s o f t h e m a n o r f r o m a b o u t t h e n o r m a n p r e f i x d i s t i n g u i s h e s a s h b y f r o m o t h e r t o w n s o f s i m i l a r n a m e p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e a s h b y d e l a z o u c h i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e a s h b y d e l a z o u c h p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e a s h b y d e l a z o u c h h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e g r e a t e a s t o n i m g a l t r i v e r w e l l a n d n e a r g r e a t e a s t o n c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e g r e a t e a s t o n g r e a t e a s t o n a h p c l a s s s m a l l p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e g r e a t e a s t o n i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e g r e a t e a s t o n p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e g r e a t e a s t o n h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e j o h n o g a u n t i m g a l t v i a d u c t n e a r j o h n o g a u n t l e i c e s t e r s h i r e c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e j o h n o g a u n t j o h n o g a u n t a h p c l a s s s m a l l p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e j o h n o g a u n t i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e j o h n o g a u n t p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e j o h n o g a u n t h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r i m g a l t c a s t l e v i e w a t n i g h t c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r l e i c e s t e r a h p c l a s s s m a l l l e i c e s t e r i s f a m e d a m o n g s t o t h e r t h i n g s f o r i t s f i n e p a r i s h c h u r c h w h i c h w a s r a i s e d t o c a t h e d r a l s t a t u s i n p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e l o u g h b o r o u g h i m g a l t t o w n h a l l i n l o u g h b o r o u g h l e i c e s t e r s h i r e c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e l o u g h b o r o u g h l o u g h b o r o u g h a h p c l a s s s m a l l t h e j o y o u s s o u n d o f b e l l s m a d e i n l o u g h b o r o u g h h a v e b e e n r u n g a l l o v e r t h e w o r l d s i n c e t h e d a t e t h a t j o h n t a y l o r o f o x f o r d m o v e d h i s f o u n d r y t o l o u g h b o r o u g h m a n y f a m o u s p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e l o u g h b o r o u g h i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e l o u g h b o r o u g h p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e l o u g h b o r o u g h h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e n e w t o w n l i n f o r d i m g a l t n e w t o w n l i n f o r d p a r i s h c h u r c h c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e n e w t o w n l i n f o r d n e w t o w n l i n f o r d a h p c l a s s s m a l l t h i s p i c t u r e s q u e v i l l a g e i s s i t u a t e d i n t h e c h a r n w o o d f o r e s t a r e a o f l e i c e s t e r s h i r e p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e n e w t o w n l i n f o r d i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e n e w t o w n l i n f o r d p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e n e w t o w n l i n f o r d h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e r o t h l e y i m g a l t k i n c h l e y l a n e c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e r o t h l e y r o t h l e y a h p c l a s s s m a l l r o t h l e y i s a d e l i g h t f u l v i l l a g e w i t h t w o v i l l a g e g r e e n s o n e i s f l a n k e d b y s o m e o f t h e c o u n t r y s f i n e s t t i m b e r f r a m e d h o u s e s t h e v i l l a g e s h o w s a g r a c e f u l s e l e c t i o n o f a r c h i t e c t u r e i n c l u d i n g p r e t t y c r u c k c o t t a g e s p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e r o t h l e y i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e r o t h l e y p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e r o t h l e y h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e t i l t o n o n t h e h i l l i m g a l t t h e f i n e o l d c h u r c h a t t i l t o n o n t h e h i l l l e i c e s t e r s h i r e c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e t i l t o n o n t h e h i l l t i l t o n o n t h e h i l l a h p c l a s s s m a l l t h i s i s a q u i e t r u r a l v i l l a g e s i t u a t e d i n o n e o f t h e h i g h e s t p a r t s o f t h e r o l l i n g l e i c e s t e r s h i r e c o u n t r y s i d e p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e t i l t o n o n t h e h i l l i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e t i l t o n o n t h e h i l l p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e t i l t o n o n t h e h i l l h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e w i s t o w i m g a l t w i s t o w l e i c e s t e r s h i r e f r o m t h e g r a n d u n i o n c a n a l c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e w i s t o w w i s t o w a h p c l a s s s m a l l p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e w i s t o w i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e w i s t o w p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e w i s t o w h o t e l s h o t e l s a p t d t r t a b l e b r a h r e f e n g l a n d l e i c e s t e r s h i r e t o w n s h i s t o r i c m a r k e t t o w n s h i s t o r i c m a r k e t t o w n s i n l e i c e s t e r s h i r e a b r a h r e f e n g l a n d l e i c e s t e r s h i r e t o w n s p i c t u r e s q u e v i l l a g e s p i c t u r e s q u e v i l l a g e s i n l e i c e s t e r s h i r e a b r a h r e f e n g l a n d l e i c e s t e r s h i r e t o w n s h i s t o r i c c i t i e s h i s t o r i c c i t i e s i n l e i c e s t e r s h i r e a b r a h r e f e n g l a n d l e i c e s t e r s h i r e t o w n s a l l t o w n s i n l e i c e s t e r s h i r e a b r a h r e f e n g l a n d t o w n s c o m p l e t e a t o z o f t o w n s i n e n g l a n d a b r b r a n a m e r e c o m m e n d e d a t t r a c t i o n s a h r e c o m m e n d e d a t t r a c t i o n s i n l e i c e s t e r s h i r e h t a b l e b o r d e r c e l l p a d d i n g c e l l s p a c i n g w i d t h t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e w o o d h o u s e e a v e s b e a c o n h i l l c o u n t r y p a r k i m g a l t v i e w f r o m b e a c o n h i l l c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e w o o d h o u s e e a v e s b e a c o n h i l l c o u n t r y p a r k b e a c o n h i l l c o u n t r y p a r k a h p c l a s s s m a l l t h e s e c o n d h i g h e s t p o i n t i n l e i c e s t e r s h i r e a t m f e e t a n d c o n t a i n i n g a b r o n z e a g e h i l l f o r t b e a c o n h i l l c o u n t r y p a r k p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e w o o d h o u s e e a v e s b e a c o n h i l l c o u n t r y p a r k i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e w o o d h o u s e e a v e s b e a c o n h i l l c o u n t r y p a r k p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e w o o d h o u s e e a v e s b e a c o n h i l l c o u n t r y p a r k h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l i n c o l n s h i r e g r a n t h a m b e l v o i r c a s t l e i m g a l t b e l v o i r c a s t l e l e i c e s t e r s h i r e c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l i n c o l n s h i r e g r a n t h a m b e l v o i r c a s t l e b e l v o i r c a s t l e a h p c l a s s s m a l l a c a s t l e h a s s t o o d o n t h i s s p o t s i n c e t h e t h c e n t u r y i t w a s b u i l t b y r o b e r t d e t o d i n i s t a n d a r d b e a r e r t o w i l l i a m t h e p p c l a s s s m a l l a h r e f e n g l a n d l i n c o l n s h i r e g r a n t h a m b e l v o i r c a s t l e i n f o r m a t i o n a | a h r e f e n g l a n d l i n c o l n s h i r e g r a n t h a m b e l v o i r c a s t l e p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l i n c o l n s h i r e g r a n t h a m b e l v o i r c a s t l e h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r b r a d g a t e p a r k i m g a l t b r a d g a t e p a r k c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r b r a d g a t e p a r k b r a d g a t e p a r k a h p c l a s s s m a l l c o m p r i s i n g o f a c r e s o f l a n d w i t h r o a m i n g d e e r b r a d g a t e p a r k i s l e i c e s t e r s h i r e s l a r g e s t a n d m o s t p o p u l a r c o u n t r y p a r k i n p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r b r a d g a t e p a r k i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r b r a d g a t e p a r k p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r b r a d g a t e p a r k h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e m a r k e t b o s w o r t h m a r k e t b o s w o r t h c o u n t r y p a r k i m g a l t m a r k e t b o s w o r t h c o u n t r y p a r k c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e m a r k e t b o s w o r t h m a r k e t b o s w o r t h c o u n t r y p a r k m a r k e t b o s w o r t h c o u n t r y p a r k a h p c l a s s s m a l l a l a n d s c a p e d h e c t a r e p a r k w h i c h w a s f o r m e r l y p a r t o f b o s w o r t h h a l l d e e r p a r k l a n d t h e r e a r e f i n e m a t u r e t r e e s a l a k e a p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e m a r k e t b o s w o r t h m a r k e t b o s w o r t h c o u n t r y p a r k i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e m a r k e t b o s w o r t h m a r k e t b o s w o r t h c o u n t r y p a r k p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e m a r k e t b o s w o r t h m a r k e t b o s w o r t h c o u n t r y p a r k h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r o l d j o h n t o w e r i m g a l t b r a d g a t e p a r k n e a r l e i c e s t e r o l d j o h n t o w e r c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r o l d j o h n t o w e r o l d j o h n t o w e r a h p c l a s s s m a l l s t a n d i n g o n a h i l l o v e r l o o k i n g a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r b r a d g a t e p a r k b r a d g a t e p a r k a o l d j o h n t o w e r f o l l y p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r o l d j o h n t o w e r i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r o l d j o h n t o w e r p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r o l d j o h n t o w e r h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e i b s t o c k s e n c e v a l l e y f o r e s t p a r k i m g a l t g r e a t c r e s t e d g r e b e c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e i b s t o c k s e n c e v a l l e y f o r e s t p a r k s e n c e v a l l e y f o r e s t p a r k a h p c l a s s s m a l l p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e i b s t o c k s e n c e v a l l e y f o r e s t p a r k i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e i b s t o c k s e n c e v a l l e y f o r e s t p a r k p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e i b s t o c k s e n c e v a l l e y f o r e s t p a r k h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e s t a u n t o n h a r o l d s t a u n t o n h a r o l d h a l l i m g a l t s t a u n t o n h a r o l d h a l l t a k e n f r o m w o o d l a n d s n e x t t o c h u r c h c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e s t a u n t o n h a r o l d s t a u n t o n h a r o l d h a l l s t a u n t o n h a r o l d h a l l a h p c l a s s s m a l l t h i s i s s a i d t o b e o n e o f t h e m o s t d e l i g h t f u l c o u n t r y h o u s e s i n l e i c e s t e r s h i r e i t w a s t h e f o r m e r h o m e o f t h e f e r r e r s f a m i l y w h o p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e s t a u n t o n h a r o l d s t a u n t o n h a r o l d h a l l i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e s t a u n t o n h a r o l d s t a u n t o n h a r o l d h a l l p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e s t a u n t o n h a r o l d s t a u n t o n h a r o l d h a l l h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r t h e j e w r y w a l l m u s e u m i m g a l t j e w r y w a l l a s m a l l p i e c e o f t h e r o m a n w a l l s t i l l r e m a i n s i n l e i c e s t e r c i t y c e n t r e c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r t h e j e w r y w a l l m u s e u m t h e j e w r y w a l l m u s e u m a h p c l a s s s m a l l t h i s i s l e i c e s t e r s l e a d i n g m u s e u m o f a r c h a e o l o g y s h o w i n g a n e x t e n s i v e r a n g e r o m a n f i n d s a s w e l l a s o t h e r u n i q u e h i s t o r i c r e l i c s p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r t h e j e w r y w a l l m u s e u m i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r t h e j e w r y w a l l m u s e u m p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r t h e j e w r y w a l l m u s e u m h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e t w y c r o s s t w y c r o s s z o o i m g a l t t w y c r o s s z o o l e i c e s t e r s h i r e s e a l i o n s c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e t w y c r o s s t w y c r o s s z o o t w y c r o s s z o o a h p c l a s s s m a l l c o m e f a c e t o f a c e w i t h o v e r a n i m a l s o f m o r e t h a n s p e c i e s i n c l u d i n g s o m e o f t h e m o s t e n d a n g e r e d o n o u r p l a n e t a t p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e t w y c r o s s t w y c r o s s z o o i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e t w y c r o s s t w y c r o s s z o o p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e t w y c r o s s t w y c r o s s z o o h o t e l s h o t e l s a p t d t r t r t d a l i g n c e n t e r w i d t h a h r e f e n g l a n d l e i c e s t e r s h i r e t h u r m a s t o n w a t e r m e a d c o u n t r y p a r k i m g a l t g r e y h e r o n c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a t d t d v a l i g n t o p h s t y l e m a r g i n b o t t o m p x a h r e f e n g l a n d l e i c e s t e r s h i r e t h u r m a s t o n w a t e r m e a d c o u n t r y p a r k w a t e r m e a d c o u n t r y p a r k a h p c l a s s s m a l l p p c l a s s s m a l l a h r e f e n g l a n d l e i c e s t e r s h i r e t h u r m a s t o n w a t e r m e a d c o u n t r y p a r k i n f o r m a t i o n a | a h r e f e n g l a n d l e i c e s t e r s h i r e t h u r m a s t o n w a t e r m e a d c o u n t r y p a r k p i c t u r e s p i c t u r e s a | a h r e f e n g l a n d l e i c e s t e r s h i r e t h u r m a s t o n w a t e r m e a d c o u n t r y p a r k h o t e l s h o t e l s a p t d t r t a b l e b r a h r e f e n g l a n d l e i c e s t e r s h i r e a t t r a c t i o n s a l l a t t r a c t i o n s i n l e i c e s t e r s h i r e a b r a h r e f e n g l a n d a t t r a c t i o n s c o m p l e t e a t o z o f a t t r a c t i o n s i n e n g l a n d a b r b r b r b r d i v c l a s s t r a i l a h r e f e n g l a n d e n g l a n d a g t a h r e f e n g l a n d l e i c e s t e r s h i r e l e i c e s t e r s h i r e a d i v d i v d i v d i v i d f o o t e r d i v a l i g n c e n t e r s p o n s o r e d l i n k s d i v s c r i p t t y p e t e x t j a v a s c r i p t ! g o o g l e a d c l i e n t c a p u b g o o g l e f o o t e r g o o g l e a d s l o t g o o g l e a d w i d t h g o o g l e a d h e i g h t s c r i p t s c r i p t t y p e t e x t j a v a s c r i p t s r c p a g e a d g o o g l e s y n d i c a t i o n c o m p a g e a d s h o w a d s j s s c r i p t d i v d i v i d c o p y r i g h t b r p a l i g n c e n t e r c o p y p i c t u r e s o f e n g l a n d c o m a l l r i g h t s r e s e r v e d b r a h r e f t e r m s t e r m s c o n d i t i o n s a b r p a l i g n c e n t e r a h r e f h t t p w w w p e s t c o n t r o l s u p e r m a r k e t c o m f o n t c o l o r r e d b p e s t c o n t r o l s u p e r m a r k e t c o m a f o n t b p e s t c o n t r o l p r i c e c o m p a r i s o n f o r e n g l a n d w a l e s s c o t l a n d i r e l a n d n o r t h e r n i r e l a n d a n d t h e c h a n n e l i s l a n d s p d i v d i v t d t d v a l i g n t o p w i d t h d i v i d r h s h e n g l a n d f a c t s h d i v c l a s s b o x s s c r i p t t y p e t e x t j a v a s c r i p t j q u e r y n o c o n f l i c t j q u e r y d o c u m e n t r e a d y f u n c t i o n j q u e r y s l i d i n g d i v h i d e j q u e r y s h o w h i d e s h o w j q u e r y s h o w h i d e c l i c k f u n c t i o n j q u e r y s l i d i n g d i v s l i d e t o g g l e s c r i p t d i v c l a s s f a c t s b o x p s t r o n g d i d y o u k n o w s t r o n g a h r e f h t t p w w w p i c t u r e s o f e n g l a n d c o m e n g l a n d n o t t i n g h a m s h i r e n o t t i n g h a m o n e o f t h e w o r l d s m o s t p o p u l a r p a i n k i l l i n g d r u g s i b u p r o f e n w a s d i s c o v e r e d i n n o t t i n g h a m e n g l a n d b y d r s t e w a r t a d a m s w h o f i r s t t e s t e d i t o n h i m s e l f w h i l s t s u f f e r i n g a h a n g o v e r a p a h r e f # c l a s s s h o w h i d e a d d f a c t a d i v c l a s s s l i d i n g d i v p f o r m m e t h o d p o s t a c t i o n o n s u b m i t i f ! t h i s s u b f a c t v a l u e a l e r t p l e a s e e n t e r y o u r f a c t r e t u r n f a l s e r e t u r n t r u e i n p u t t y p e h i d d e n n a m e a c t i o n v a l u e a d d f a c t t e x t a r e a s t y l e w i d t h p x h e i g h t p x r o w s c o l s n a m e s u b f a c t t e x t a r e a i n p u t t y p e s u b m i t n a m e s u b m i t v a l u e a d d f a c t s t y l e w i d t h p x m a r g i n l e f t p x f o r m p a h r e f # c l a s s s h o w h i d e h i d e a d i v d i v d i v h m e m b e r c r e a t e d t o u r s h d i v c l a s s b o x s c b r p a h r e f u s e r m e m i t o u r l a n e s a n d f o o t p a t h s i m g a l t m i s t y m o r n i n g i n a b r a h a m s v a l l e y c a n n o c k c h a s e s t a f f o r d s h i r e c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a b r s t r o n g l a n e s a n d f o o t p a t h s s t r o n g b r b y a h r e f u s e r m e m i t o u r s m a r j o r i e p o p e a p p a h r e f u s e r s t e p h j t o u r y e l l o w f i e l d s i m g a l t g o l d e n g u i s b o r o u g h c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a b r s t r o n g y e l l o w f i e l d s s t r o n g b r b y a h r e f u s e r s t e p h j t o u r s s t e p h a n i e j a c k s o n a p p a h r e f u s e r h a n s t o u r s i s s i n g h u r s t c a s t l e g a r d e n i m g a l t h o u s e v i e w c l a s s i m a g e s s r c i m g s j p g w i d t h h e i g h t b o r d e r a b r s t r o n g s i s s i n g h u r s t c a s t l e g a r d e n s t r o n g b r b y a h r e f u s e r h a n s t o u r s h a n s l i t t e l a p d i v h s p o n s o r e d l i n k s h d i v c l a s s b o x s c s c r i p t t y p e t e x t j a v a s c r i p t ! g o o g l e a d c l i e n t c a p u b s i t e w i d e s k y s c r a p e r g o o g l e a d s l o t g o o g l e a d w i d t h g o o g l e a d h e i g h t s c r i p t s c r i p t t y p e t e x t j a v a s c r i p t s r c h t t p p a g e a d g o o g l e s y n d i c a t i o n c o m p a g e a d s h o w a d s j s s c r i p t d i v a l i g n c e n t e r a h r e f h t t p w w w a m a z o n c o m d p b d v r j m i m g s r c b a n n e r s c s l e w i s e b o o k j p g a d i v d i v a l i g n c e n t e r a h r e f h t t p s o u t h c o a s t c a m p s i t e s c o m i m g s r c b a n n e r s s c c l o g o j p g a d i v d i v d i v t d t r t a b l e d i v d i v d i v d i v d i v d i v ! g e n e r a t e d b y a t i n s e c o n d s q u e r i e s t o o k s e c o n d s b o d y h t m l ')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#problematic example\n",
    "input_df.select(\"id\",\"text\").where(input_df.id == 1310562).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare feature and label dataframe\n",
    "\n",
    "So far, the features and labels were in 2 separate dataframes (the raw data came in without labels, and labels were associated with filenames). We are going to need to have both in one dataframe.\n",
    "\n",
    "This is very easy to achieve via a foolowing join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = false)\n",
      " |-- images: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- links: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- text: string (nullable = false)\n",
      " |-- title: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_wlabels_df = input_df.join(train_label_df,\"id\")\n",
    "train_wlabels_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap and stratified sampling\n",
    "\n",
    "In this example, we are dealing with a 2 class classification problem: possible labels are 0 or 1. Let us take a look at the fractions of these labels:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.971772880251873"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_wlabels_df.where(train_wlabels_df.label == 1).count()/float(train_wlabels_df.count())*100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.02822711974812"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_wlabels_df.where(train_wlabels_df.label == 0).count()/float(train_wlabels_df.count())*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that label 1 is underrepresented, comprisoing just about 10%.\n",
    "\n",
    "Normally we need to use stratification (stratified sampling) or bootstrap sampling when working with such unbalanced data. In short, what these methods do is they look at the class labels on the training/cross validation stage, and try to prepare a sample which would have a distribution similar to what one could draw from the whole population. With bootstraping one can change the fractions of classes in the sample.\n",
    "\n",
    "\n",
    "If you are working with RDDs of key-value pairs (pair RDDs), you can use stratified sampling methods like **sampleByKey** and **sampleByKeyExact**. For stratified sampling, the keys can be thought of as a label and the value as a specific attribute. The sampleByKey method will flip a coin to decide whether an observation will be sampled or not, therefore requires one pass over the data, and provides an expected sample size. sampleByKeyExact requires significant more resources than the per-stratum simple random sampling used in sampleByKey, but will provide the exact sampling size with 99.99% confidence. sampleByKeyExact is currently not supported in python.\n",
    "\n",
    "The sampleByKey and sampleByKeyExact functions take **withReplacement** parameter. Posisson or Bernoulli sampling function is used depending on that flag:\n",
    "\n",
    "```scala\n",
    "    val samplingFunc = if (withReplacement) {\n",
    "      StratifiedSamplingUtils.getPoissonSamplingFunction(self, fractions, false, seed)\n",
    "    } else {\n",
    "      StratifiedSamplingUtils.getBernoulliSamplingFunction(self, fractions, false, seed)\n",
    "    }\n",
    "```    \n",
    " \n",
    "With DataFrames, only the sampleBy method is currently available, and a subset of rows of the initial dataframe is with a specific label is selected by sampling random uniform distribution.\n",
    "\n",
    "For our exercises, we are just going to play with the fraction of the larger class and see an advnatage of downsampling it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1 is under represented class\n",
    "fractions = {1.0:1.0, 0.0:0.5}\n",
    "stratified = train_wlabels_df.sampleBy(\"label\", fractions, 36L)\n",
    "\n",
    "train, cv = stratified.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Most of our features are text. There are several common types of features and approaches that form the \"starting point\" for NLP and text-based data problems. Here are a few of the common ones:\n",
    "\n",
    "> Bag of words\n",
    "\n",
    "> TF/IDF\n",
    "\n",
    "> n-grams\n",
    "\n",
    "> Stemming / part of speech tagging / etc.\n",
    "\n",
    "> Feature hashing\n",
    "\n",
    "## Some useful Python tools are:\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "> http://www.nltk.org/\n",
    "\n",
    "> http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "\n",
    "## Prepare text features\n",
    "\n",
    "We are going to try using all of the above techqniues, but with **spark.ml**\n",
    "\n",
    "### Extract tokens\n",
    "\n",
    "Our first step is to tokenize data. The simple way to tokenize a string (sentence) in Python is using the split() method as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fruit', 'eat', 'tasty', 'pie', 'leaf', 'cook', 'tree', 'computer', 'computers', 'laptop', 'tech', 'technology', 'ceo', 'jobs', 'ipad', 'iphone', 'announce', 'announced', 'mac', 'company', 'companies', 'employee', 'employees', 'user', 'software', 'released']\n"
     ]
    }
   ],
   "source": [
    "input_document = \"fruit eat tasty pie leaf cook tree computer computers laptop tech technology ceo jobs ipad iphone announce announced mac company companies employee employees user software released\"\n",
    "tokens = input_document.split(\" \")\n",
    "print tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spakr ML has 2 tokenizers: default Tokenizer and RegexTokenizer which allows to specify a custom pattern to tokenize on. The latter is more flexible, we are going to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare text features...\n",
      "+--------------------+\n",
      "|               words|\n",
      "+--------------------+\n",
      "|[insolite, art, e...|\n",
      "|[israeli, missile...|\n",
      "|[the, page, you, ...|\n",
      "|[advertisement, b...|\n",
      "|[make, more, mone...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, Tokenizer\n",
    "\n",
    "print \"Prepare text features...\"\n",
    "#tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenized_df = tokenizer.transform(train_wlabels_df)\n",
    "tokenized_df.select(\"words\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=[u'insolite', u'art', u'et', u'autres', u'cest', u'une', u'srie', u'de', u'clichs', u'tonnants', u'pris', u'par', u'rian', u'van', u'schalkwyk', u'dans', u'lesquels', u'un', u'lphant', u'retourne', u'un', u'hippopotame', u'avec', u'sa', u'trompe', u'alors', u'quil', u'djeunait', u'dans', u'le', u'restaurant', u'de', u'la', u'rserve', u'private', u'game', u'erindi', u'en', u'namibie', u'ce', u'mdecin', u'observait', u'un', u'troupeau', u'dune', u'vingtaine', u'dhippopotames', u'qui', u'mangeaient', u'en', u'contrebas', u'dans', u'un', u'troue', u'deau', u'la', u'saison', u'tant', u'sche', u'le', u'personnel', u'leur', u'avait', u'apport', u'un', u'peu', u'dherbe', u'pour', u'pallier', u'au', u'manque', u'de', u'nourriture', u'cest', u'alors', u'quun', u'lphant', u'a', u'surgit', u'de', u'la', u'brousse', u'pour', u'profiter', u'de', u'laubaine', u'il', u'a', u'commenc', u'manger', u'tranquillement', u'puis', u'est', u'devenu', u'plus', u'agit', u'la', u'prsence', u'des', u'hippopotames', u'alors', u'quun', u'petit', u'passait', u'proximit', u'une', u'maman', u'hippopotame', u'sest', u'interpose', u'et', u'a', u'subit', u'la', u'charge', u'de', u'lanimal', u'llphant', u'a', u'alors', u'retourn', u'en', u'lair', u'la', u'maman', u'hippopotame', u'avec', u'sa', u'trompe', u'et', u'elle', u'est', u'lourdement', u'tonnes', u'la', u'bte', u'quand', u'mme', u'retombe', u'sur', u'le', u'dos', u'elle', u'sen', u'sortira', u'toutefois', u'avec', u'seulement', u'quelques', u'gratignures', u'voil', u'enfin', u'la', u'rponse', u'la', u'question', u'pose', u'par', u'simon', u'dans', u'la', u'cit', u'de', u'la', u'peur', u'votre', u'avis', u'cest', u'qui', u'le', u'plus', u'fort', u'lhippopotame', u'ou', u'llphant', u'plus', u'dinfos', u'sur', u'la', u'source', u'de', u'ce', u'post', u'ici', u'tags', u'bagarre', u'bb', u'elephant', u'hippopotame', u'image', u'photo', u'trompe', u'pingback', u'un', u'buffle', u'projette', u'un', u'lion', u'en', u'lair', u'avec', u'ses', u'cornes', u'toutrien', u'author', u'required', u'email', u'will', u'not', u'be', u'publishedrequired', u'website', u'notifiezmoi', u'des', u'commentaires', u'venir', u'via', u'mail', u'vous', u'pouvez', u'aussi', u'vous', u'abonner', u'sans', u'commenter', u'newsletter', u'entrez', u'votre', u'email']),\n",
       " Row(words=[u'israeli', u'missile', u'notification', u'service', u'red', u'alert', u'has', u'teamed', u'up', u'with', u'the', u'unlikely', u'hit', u'app', u'yo', u'to', u'warn', u'of', u'incoming', u'strikes', u'leaving', u'many', u'in', u'the', u'local', u'tech', u'scene', u'more', u'bemused', u'than', u'impressed', u'yo', u'users', u'who', u'follow', u'redalertisrael', u'within', u'the', u'app', u'receive', u'an', u'alert', u'whenever', u'spotted', u'attacks', u'are', u'inbound', u'red', u'alert', u'provides', u'realtime', u'warnings', u'of', u'mortars', u'or', u'missiles', u'fired', u'into', u'israel', u'by', u'palestinian', u'militants', u'some', u'experts', u'remain', u'sceptical', u'about', u'the', u'usefulness', u'of', u'yo', u'the', u'app', u'developed', u'by', u'a', u'san', u'franciscobased', u'israeli', u'sends', u'the', u'word', u'yo', u'as', u'a', u'text', u'and', u'audio', u'notification', u'to', u'friends', u'but', u'nothing', u'else', u'since', u'launching', u'in', u'april', u'it', u'has', u'been', u'downloaded', u'about', u'two', u'million', u'times', u'across', u'the', u'globe', u'the', u'creators', u'of', u'red', u'alert', u'ari', u'sprung', u'and', u'kobi', u'snir', u'aim', u'to', u'use', u'yos', u'simple', u'push', u'notification', u'service', u'to', u'reach', u'out', u'to', u'a', u'larger', u'pool', u'of', u'citizens', u'who', u'may', u'be', u'at', u'risk', u'from', u'attack', u'according', u'the', u'times', u'of', u'israel', u'the', u'yo', u'notifications', u'are', u'meant', u'to', u'complement', u'the', u'red', u'alert', u'app', u'which', u'has', u'a', u'more', u'comprehensive', u'breakdown', u'of', u'the', u'imminent', u'threats', u'mr', u'sprung', u'told', u'the', u'times', u'of', u'israel', u'that', u'his', u'app', u'gets', u'its', u'information', u'from', u'the', u'israel', u'defense', u'force', u'and', u'the', u'homefront', u'command', u'dvir', u'reznik', u'an', u'advisor', u'for', u'startups', u'in', u'israel', u'told', u'the', u'bbc', u'that', u'the', u'yo', u'alerts', u'seemed', u'halfbaked', u'if', u'im', u'on', u'one', u'side', u'of', u'israel', u'and', u'a', u'missile', u'lands', u'miles', u'away', u'the', u'yo', u'warning', u'is', u'not', u'of', u'much', u'use', u'to', u'me', u'he', u'said', u'however', u'its', u'better', u'than', u'nothing', u'and', u'i', u'can', u'see', u'this', u'evolving', u'into', u'something', u'more', u'meaningful', u'but', u'it', u'needs', u'more', u'substance', u'to', u'make', u'it', u'as', u'convenient', u'and', u'as', u'useful', u'as', u'red', u'alert', u'the', u'retooling', u'of', u'yo', u'comes', u'at', u'a', u'time', u'of', u'increased', u'tension', u'between', u'israel', u'and', u'the', u'palestinians', u'there', u'are', u'no', u'similar', u'apps', u'in', u'gaza', u'with', u'many', u'palestinians', u'relying', u'instead', u'on', u'twitter', u'hashtags', u'to', u'avoid', u'danger', u'yo', u'was', u'recently', u'derided', u'by', u'critics', u'as', u'being', u'a', u'gimmick', u'and', u'accelerating', u'the', u'decline', u'of', u'humanity', u'yaniv', u'feldman', u'of', u'israelbased', u'tech', u'website', u'geektime', u'told', u'the', u'bbc', u'that', u'he', u'thought', u'yo', u'was', u'one', u'of', u'the', u'dumbest', u'ideas', u'ever', u'and', u'that', u'the', u'only', u'beneficial', u'use', u'for', u'the', u'yo', u'and', u'red', u'alert', u'app', u'mashup', u'was', u'to', u'raise', u'awareness', u'among', u'people', u'who', u'are', u'not', u'in', u'israel', u'and', u'do', u'not', u'realise', u'how', u'often', u'missile', u'strikes', u'affect', u'us', u'anouk', u'lorie', u'editor', u'in', u'chief', u'of', u'nocamels', u'an', u'israeli', u'tech', u'blog', u'was', u'however', u'a', u'little', u'more', u'positive', u'red', u'alert', u'is', u'a', u'potentially', u'lifesaving', u'app', u'for', u'a', u'large', u'number', u'of', u'israelis', u'who', u'have', u'only', u'seconds', u'to', u'find', u'cover', u'from', u'the', u'dozens', u'of', u'daily', u'incoming', u'rockets', u'so', u'im', u'not', u'sure', u'that', u'yo', u'is', u'the', u'appropriate', u'word', u'to', u'see', u'pop', u'up', u'on', u'mobiles', u'she', u'said', u'on', u'the', u'other', u'hand', u'perhaps', u'seeing', u'yo', u'on', u'your', u'screen', u'feels', u'less', u'distressing', u'than', u'the', u'loud', u'siren', u'that', u'is', u'otherwise', u'the', u'default', u'on', u'red', u'alert', u'however', u'no', u'matter', u'how', u'pointless', u'some', u'people', u'deem', u'the', u'yo', u'app', u'if', u'it', u'can', u'even', u'just', u'slightly', u'lighten', u'an', u'otherwise', u'very', u'difficult', u'situation', u'i', u'welcome', u'it', u'ari', u'sprung', u'coauthor', u'of', u'red', u'alert', u'told', u'the', u'bbc', u'that', u'the', u'mashup', u'is', u'definitely', u'giving', u'the', u'rest', u'of', u'the', u'world', u'a', u'sense', u'of', u'the', u'volume', u'of', u'missiles', u'that', u'are', u'being', u'launched', u'into', u'israel', u'on', u'an', u'hourly', u'basis', u'and', u'that', u'he', u'hopes', u'the', u'app', u'will', u'raise', u'awareness', u'the', u'bbc', u'is', u'not', u'responsible', u'for', u'the', u'content', u'of', u'external', u'internet', u'sites', u'after', u'years', u'of', u'negotiations', u'world', u'powers', u'reach', u'a', u'deal', u'with', u'iran', u'on', u'limiting', u'iranian', u'nuclear', u'activity', u'in', u'return', u'for', u'the', u'lifting', u'of', u'sanctions', u'was', u'this', u'man', u'a', u'bank', u'clerk', u'or', u'a', u'brilliant', u'spy', u'what', u'are', u'the', u'best', u'ways', u'to', u'fight', u'memory', u'loss', u'burt', u'bacharach', u'on', u'having', u'his', u'hits', u'reimagined', u'how', u'an', u'italian', u'ancestor', u'gave', u'a', u'syrian', u'family', u'a', u'ticket', u'to', u'safety', u'the', u'plant', u'thats', u'been', u'used', u'as', u'a', u'medicine', u'and', u'a', u'murder', u'weapon', u'leftwing', u'party', u'sits', u'on', u'mplus', u'property', u'asset', u'fighting', u'the', u'denial', u'stopping', u'the', u'end', u'of', u'ebola', u'the', u'sinister', u'roots', u'of', u'the', u'word', u'surveillance', u'famous', u'toy', u'shop', u'closes', u'in', u'new', u'york'])]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df.select(\"words\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "Instead of looking at just single words, it is also useful to look at n-grams: these are n-word long sequences of words (i.e., each of \"farmer's market\", \"market share\", and \"farm share\" is a 2-gram).\n",
    "The exact same tokenization techniques apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try ngrams instead, or in addition...\n",
      "+--------------------+\n",
      "|              ngrams|\n",
      "+--------------------+\n",
      "|[insolite art, ar...|\n",
      "|[israeli missile,...|\n",
      "|[the page, page y...|\n",
      "|[advertisement bd...|\n",
      "|[make more, more ...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "print \"Try ngrams instead, or in addition...\"\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngram_df = ngram.transform(tokenized_df)\n",
    "ngram_df.select(\"ngrams\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ngrams=[u'insolite art', u'art et', u'et autres', u'autres cest', u'cest une', u'une srie', u'srie de', u'de clichs', u'clichs tonnants', u'tonnants pris', u'pris par', u'par rian', u'rian van', u'van schalkwyk', u'schalkwyk dans', u'dans lesquels', u'lesquels un', u'un lphant', u'lphant retourne', u'retourne un', u'un hippopotame', u'hippopotame avec', u'avec sa', u'sa trompe', u'trompe alors', u'alors quil', u'quil djeunait', u'djeunait dans', u'dans le', u'le restaurant', u'restaurant de', u'de la', u'la rserve', u'rserve private', u'private game', u'game erindi', u'erindi en', u'en namibie', u'namibie ce', u'ce mdecin', u'mdecin observait', u'observait un', u'un troupeau', u'troupeau dune', u'dune vingtaine', u'vingtaine dhippopotames', u'dhippopotames qui', u'qui mangeaient', u'mangeaient en', u'en contrebas', u'contrebas dans', u'dans un', u'un troue', u'troue deau', u'deau la', u'la saison', u'saison tant', u'tant sche', u'sche le', u'le personnel', u'personnel leur', u'leur avait', u'avait apport', u'apport un', u'un peu', u'peu dherbe', u'dherbe pour', u'pour pallier', u'pallier au', u'au manque', u'manque de', u'de nourriture', u'nourriture cest', u'cest alors', u'alors quun', u'quun lphant', u'lphant a', u'a surgit', u'surgit de', u'de la', u'la brousse', u'brousse pour', u'pour profiter', u'profiter de', u'de laubaine', u'laubaine il', u'il a', u'a commenc', u'commenc manger', u'manger tranquillement', u'tranquillement puis', u'puis est', u'est devenu', u'devenu plus', u'plus agit', u'agit la', u'la prsence', u'prsence des', u'des hippopotames', u'hippopotames alors', u'alors quun', u'quun petit', u'petit passait', u'passait proximit', u'proximit une', u'une maman', u'maman hippopotame', u'hippopotame sest', u'sest interpose', u'interpose et', u'et a', u'a subit', u'subit la', u'la charge', u'charge de', u'de lanimal', u'lanimal llphant', u'llphant a', u'a alors', u'alors retourn', u'retourn en', u'en lair', u'lair la', u'la maman', u'maman hippopotame', u'hippopotame avec', u'avec sa', u'sa trompe', u'trompe et', u'et elle', u'elle est', u'est lourdement', u'lourdement tonnes', u'tonnes la', u'la bte', u'bte quand', u'quand mme', u'mme retombe', u'retombe sur', u'sur le', u'le dos', u'dos elle', u'elle sen', u'sen sortira', u'sortira toutefois', u'toutefois avec', u'avec seulement', u'seulement quelques', u'quelques gratignures', u'gratignures voil', u'voil enfin', u'enfin la', u'la rponse', u'rponse la', u'la question', u'question pose', u'pose par', u'par simon', u'simon dans', u'dans la', u'la cit', u'cit de', u'de la', u'la peur', u'peur votre', u'votre avis', u'avis cest', u'cest qui', u'qui le', u'le plus', u'plus fort', u'fort lhippopotame', u'lhippopotame ou', u'ou llphant', u'llphant plus', u'plus dinfos', u'dinfos sur', u'sur la', u'la source', u'source de', u'de ce', u'ce post', u'post ici', u'ici tags', u'tags bagarre', u'bagarre bb', u'bb elephant', u'elephant hippopotame', u'hippopotame image', u'image photo', u'photo trompe', u'trompe pingback', u'pingback un', u'un buffle', u'buffle projette', u'projette un', u'un lion', u'lion en', u'en lair', u'lair avec', u'avec ses', u'ses cornes', u'cornes toutrien', u'toutrien author', u'author required', u'required email', u'email will', u'will not', u'not be', u'be publishedrequired', u'publishedrequired website', u'website notifiezmoi', u'notifiezmoi des', u'des commentaires', u'commentaires venir', u'venir via', u'via mail', u'mail vous', u'vous pouvez', u'pouvez aussi', u'aussi vous', u'vous abonner', u'abonner sans', u'sans commenter', u'commenter newsletter', u'newsletter entrez', u'entrez votre', u'votre email']),\n",
       " Row(ngrams=[u'israeli missile', u'missile notification', u'notification service', u'service red', u'red alert', u'alert has', u'has teamed', u'teamed up', u'up with', u'with the', u'the unlikely', u'unlikely hit', u'hit app', u'app yo', u'yo to', u'to warn', u'warn of', u'of incoming', u'incoming strikes', u'strikes leaving', u'leaving many', u'many in', u'in the', u'the local', u'local tech', u'tech scene', u'scene more', u'more bemused', u'bemused than', u'than impressed', u'impressed yo', u'yo users', u'users who', u'who follow', u'follow redalertisrael', u'redalertisrael within', u'within the', u'the app', u'app receive', u'receive an', u'an alert', u'alert whenever', u'whenever spotted', u'spotted attacks', u'attacks are', u'are inbound', u'inbound red', u'red alert', u'alert provides', u'provides realtime', u'realtime warnings', u'warnings of', u'of mortars', u'mortars or', u'or missiles', u'missiles fired', u'fired into', u'into israel', u'israel by', u'by palestinian', u'palestinian militants', u'militants some', u'some experts', u'experts remain', u'remain sceptical', u'sceptical about', u'about the', u'the usefulness', u'usefulness of', u'of yo', u'yo the', u'the app', u'app developed', u'developed by', u'by a', u'a san', u'san franciscobased', u'franciscobased israeli', u'israeli sends', u'sends the', u'the word', u'word yo', u'yo as', u'as a', u'a text', u'text and', u'and audio', u'audio notification', u'notification to', u'to friends', u'friends but', u'but nothing', u'nothing else', u'else since', u'since launching', u'launching in', u'in april', u'april it', u'it has', u'has been', u'been downloaded', u'downloaded about', u'about two', u'two million', u'million times', u'times across', u'across the', u'the globe', u'globe the', u'the creators', u'creators of', u'of red', u'red alert', u'alert ari', u'ari sprung', u'sprung and', u'and kobi', u'kobi snir', u'snir aim', u'aim to', u'to use', u'use yos', u'yos simple', u'simple push', u'push notification', u'notification service', u'service to', u'to reach', u'reach out', u'out to', u'to a', u'a larger', u'larger pool', u'pool of', u'of citizens', u'citizens who', u'who may', u'may be', u'be at', u'at risk', u'risk from', u'from attack', u'attack according', u'according the', u'the times', u'times of', u'of israel', u'israel the', u'the yo', u'yo notifications', u'notifications are', u'are meant', u'meant to', u'to complement', u'complement the', u'the red', u'red alert', u'alert app', u'app which', u'which has', u'has a', u'a more', u'more comprehensive', u'comprehensive breakdown', u'breakdown of', u'of the', u'the imminent', u'imminent threats', u'threats mr', u'mr sprung', u'sprung told', u'told the', u'the times', u'times of', u'of israel', u'israel that', u'that his', u'his app', u'app gets', u'gets its', u'its information', u'information from', u'from the', u'the israel', u'israel defense', u'defense force', u'force and', u'and the', u'the homefront', u'homefront command', u'command dvir', u'dvir reznik', u'reznik an', u'an advisor', u'advisor for', u'for startups', u'startups in', u'in israel', u'israel told', u'told the', u'the bbc', u'bbc that', u'that the', u'the yo', u'yo alerts', u'alerts seemed', u'seemed halfbaked', u'halfbaked if', u'if im', u'im on', u'on one', u'one side', u'side of', u'of israel', u'israel and', u'and a', u'a missile', u'missile lands', u'lands miles', u'miles away', u'away the', u'the yo', u'yo warning', u'warning is', u'is not', u'not of', u'of much', u'much use', u'use to', u'to me', u'me he', u'he said', u'said however', u'however its', u'its better', u'better than', u'than nothing', u'nothing and', u'and i', u'i can', u'can see', u'see this', u'this evolving', u'evolving into', u'into something', u'something more', u'more meaningful', u'meaningful but', u'but it', u'it needs', u'needs more', u'more substance', u'substance to', u'to make', u'make it', u'it as', u'as convenient', u'convenient and', u'and as', u'as useful', u'useful as', u'as red', u'red alert', u'alert the', u'the retooling', u'retooling of', u'of yo', u'yo comes', u'comes at', u'at a', u'a time', u'time of', u'of increased', u'increased tension', u'tension between', u'between israel', u'israel and', u'and the', u'the palestinians', u'palestinians there', u'there are', u'are no', u'no similar', u'similar apps', u'apps in', u'in gaza', u'gaza with', u'with many', u'many palestinians', u'palestinians relying', u'relying instead', u'instead on', u'on twitter', u'twitter hashtags', u'hashtags to', u'to avoid', u'avoid danger', u'danger yo', u'yo was', u'was recently', u'recently derided', u'derided by', u'by critics', u'critics as', u'as being', u'being a', u'a gimmick', u'gimmick and', u'and accelerating', u'accelerating the', u'the decline', u'decline of', u'of humanity', u'humanity yaniv', u'yaniv feldman', u'feldman of', u'of israelbased', u'israelbased tech', u'tech website', u'website geektime', u'geektime told', u'told the', u'the bbc', u'bbc that', u'that he', u'he thought', u'thought yo', u'yo was', u'was one', u'one of', u'of the', u'the dumbest', u'dumbest ideas', u'ideas ever', u'ever and', u'and that', u'that the', u'the only', u'only beneficial', u'beneficial use', u'use for', u'for the', u'the yo', u'yo and', u'and red', u'red alert', u'alert app', u'app mashup', u'mashup was', u'was to', u'to raise', u'raise awareness', u'awareness among', u'among people', u'people who', u'who are', u'are not', u'not in', u'in israel', u'israel and', u'and do', u'do not', u'not realise', u'realise how', u'how often', u'often missile', u'missile strikes', u'strikes affect', u'affect us', u'us anouk', u'anouk lorie', u'lorie editor', u'editor in', u'in chief', u'chief of', u'of nocamels', u'nocamels an', u'an israeli', u'israeli tech', u'tech blog', u'blog was', u'was however', u'however a', u'a little', u'little more', u'more positive', u'positive red', u'red alert', u'alert is', u'is a', u'a potentially', u'potentially lifesaving', u'lifesaving app', u'app for', u'for a', u'a large', u'large number', u'number of', u'of israelis', u'israelis who', u'who have', u'have only', u'only seconds', u'seconds to', u'to find', u'find cover', u'cover from', u'from the', u'the dozens', u'dozens of', u'of daily', u'daily incoming', u'incoming rockets', u'rockets so', u'so im', u'im not', u'not sure', u'sure that', u'that yo', u'yo is', u'is the', u'the appropriate', u'appropriate word', u'word to', u'to see', u'see pop', u'pop up', u'up on', u'on mobiles', u'mobiles she', u'she said', u'said on', u'on the', u'the other', u'other hand', u'hand perhaps', u'perhaps seeing', u'seeing yo', u'yo on', u'on your', u'your screen', u'screen feels', u'feels less', u'less distressing', u'distressing than', u'than the', u'the loud', u'loud siren', u'siren that', u'that is', u'is otherwise', u'otherwise the', u'the default', u'default on', u'on red', u'red alert', u'alert however', u'however no', u'no matter', u'matter how', u'how pointless', u'pointless some', u'some people', u'people deem', u'deem the', u'the yo', u'yo app', u'app if', u'if it', u'it can', u'can even', u'even just', u'just slightly', u'slightly lighten', u'lighten an', u'an otherwise', u'otherwise very', u'very difficult', u'difficult situation', u'situation i', u'i welcome', u'welcome it', u'it ari', u'ari sprung', u'sprung coauthor', u'coauthor of', u'of red', u'red alert', u'alert told', u'told the', u'the bbc', u'bbc that', u'that the', u'the mashup', u'mashup is', u'is definitely', u'definitely giving', u'giving the', u'the rest', u'rest of', u'of the', u'the world', u'world a', u'a sense', u'sense of', u'of the', u'the volume', u'volume of', u'of missiles', u'missiles that', u'that are', u'are being', u'being launched', u'launched into', u'into israel', u'israel on', u'on an', u'an hourly', u'hourly basis', u'basis and', u'and that', u'that he', u'he hopes', u'hopes the', u'the app', u'app will', u'will raise', u'raise awareness', u'awareness the', u'the bbc', u'bbc is', u'is not', u'not responsible', u'responsible for', u'for the', u'the content', u'content of', u'of external', u'external internet', u'internet sites', u'sites after', u'after years', u'years of', u'of negotiations', u'negotiations world', u'world powers', u'powers reach', u'reach a', u'a deal', u'deal with', u'with iran', u'iran on', u'on limiting', u'limiting iranian', u'iranian nuclear', u'nuclear activity', u'activity in', u'in return', u'return for', u'for the', u'the lifting', u'lifting of', u'of sanctions', u'sanctions was', u'was this', u'this man', u'man a', u'a bank', u'bank clerk', u'clerk or', u'or a', u'a brilliant', u'brilliant spy', u'spy what', u'what are', u'are the', u'the best', u'best ways', u'ways to', u'to fight', u'fight memory', u'memory loss', u'loss burt', u'burt bacharach', u'bacharach on', u'on having', u'having his', u'his hits', u'hits reimagined', u'reimagined how', u'how an', u'an italian', u'italian ancestor', u'ancestor gave', u'gave a', u'a syrian', u'syrian family', u'family a', u'a ticket', u'ticket to', u'to safety', u'safety the', u'the plant', u'plant thats', u'thats been', u'been used', u'used as', u'as a', u'a medicine', u'medicine and', u'and a', u'a murder', u'murder weapon', u'weapon leftwing', u'leftwing party', u'party sits', u'sits on', u'on mplus', u'mplus property', u'property asset', u'asset fighting', u'fighting the', u'the denial', u'denial stopping', u'stopping the', u'the end', u'end of', u'of ebola', u'ebola the', u'the sinister', u'sinister roots', u'roots of', u'of the', u'the word', u'word surveillance', u'surveillance famous', u'famous toy', u'toy shop', u'shop closes', u'closes in', u'in new', u'new york'])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_df.select(\"ngrams\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Remove stopwords\n",
    "\n",
    "It's common to want to omit certain common words when doing these counts -- \"a\", \"an\", and \"the\" are common enough so that their counts do not tend to give us any hints as to the meaning of documents. Such words that we want to omit are called stop words (they don't stop anything, though).\n",
    "\n",
    "Spark ML contains a standard list of such stop words for English. One can include any custom stopwords, if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords\n",
      "+--------------------+\n",
      "|            filtered|\n",
      "+--------------------+\n",
      "|[insolite, art, e...|\n",
      "|[israeli, missile...|\n",
      "|[page, trying, vi...|\n",
      "|[advertisement, b...|\n",
      "|[make, money, tod...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "print \"Remove stopwords\"\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filtered_df = remover.transform(tokenized_df)\n",
    "#filtered_df.printSchema()\n",
    "filtered_df.select(\"filtered\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(filtered=[u'insolite', u'art', u'et', u'autres', u'cest', u'une', u'srie', u'clichs', u'tonnants', u'pris', u'par', u'rian', u'van', u'schalkwyk', u'dans', u'lesquels', u'lphant', u'retourne', u'hippopotame', u'avec', u'sa', u'trompe', u'alors', u'quil', u'djeunait', u'dans', u'le', u'restaurant', u'la', u'rserve', u'private', u'game', u'erindi', u'en', u'namibie', u'ce', u'mdecin', u'observait', u'troupeau', u'dune', u'vingtaine', u'dhippopotames', u'qui', u'mangeaient', u'en', u'contrebas', u'dans', u'troue', u'deau', u'la', u'saison', u'tant', u'sche', u'le', u'personnel', u'leur', u'avait', u'apport', u'peu', u'dherbe', u'pour', u'pallier', u'au', u'manque', u'nourriture', u'cest', u'alors', u'quun', u'lphant', u'surgit', u'la', u'brousse', u'pour', u'profiter', u'laubaine', u'il', u'commenc', u'manger', u'tranquillement', u'puis', u'est', u'devenu', u'plus', u'agit', u'la', u'prsence', u'des', u'hippopotames', u'alors', u'quun', u'petit', u'passait', u'proximit', u'une', u'maman', u'hippopotame', u'sest', u'interpose', u'et', u'subit', u'la', u'charge', u'lanimal', u'llphant', u'alors', u'retourn', u'en', u'lair', u'la', u'maman', u'hippopotame', u'avec', u'sa', u'trompe', u'et', u'elle', u'est', u'lourdement', u'tonnes', u'la', u'bte', u'quand', u'mme', u'retombe', u'sur', u'le', u'dos', u'elle', u'sen', u'sortira', u'toutefois', u'avec', u'seulement', u'quelques', u'gratignures', u'voil', u'enfin', u'la', u'rponse', u'la', u'question', u'pose', u'par', u'simon', u'dans', u'la', u'cit', u'la', u'peur', u'votre', u'avis', u'cest', u'qui', u'le', u'plus', u'fort', u'lhippopotame', u'ou', u'llphant', u'plus', u'dinfos', u'sur', u'la', u'source', u'ce', u'post', u'ici', u'tags', u'bagarre', u'bb', u'elephant', u'hippopotame', u'image', u'photo', u'trompe', u'pingback', u'buffle', u'projette', u'lion', u'en', u'lair', u'avec', u'ses', u'cornes', u'toutrien', u'author', u'required', u'email', u'publishedrequired', u'website', u'notifiezmoi', u'des', u'commentaires', u'venir', u'mail', u'vous', u'pouvez', u'aussi', u'vous', u'abonner', u'sans', u'commenter', u'newsletter', u'entrez', u'votre', u'email']),\n",
       " Row(filtered=[u'israeli', u'missile', u'notification', u'service', u'red', u'alert', u'teamed', u'unlikely', u'hit', u'app', u'yo', u'warn', u'incoming', u'strikes', u'leaving', u'local', u'tech', u'scene', u'bemused', u'impressed', u'yo', u'users', u'follow', u'redalertisrael', u'app', u'receive', u'alert', u'spotted', u'attacks', u'inbound', u'red', u'alert', u'provides', u'realtime', u'warnings', u'mortars', u'missiles', u'fired', u'israel', u'palestinian', u'militants', u'experts', u'remain', u'sceptical', u'usefulness', u'yo', u'app', u'developed', u'san', u'franciscobased', u'israeli', u'sends', u'word', u'yo', u'text', u'audio', u'notification', u'friends', u'launching', u'april', u'downloaded', u'million', u'times', u'globe', u'creators', u'red', u'alert', u'ari', u'sprung', u'kobi', u'snir', u'aim', u'use', u'yos', u'simple', u'push', u'notification', u'service', u'reach', u'larger', u'pool', u'citizens', u'risk', u'attack', u'according', u'times', u'israel', u'yo', u'notifications', u'meant', u'complement', u'red', u'alert', u'app', u'comprehensive', u'breakdown', u'imminent', u'threats', u'mr', u'sprung', u'told', u'times', u'israel', u'app', u'gets', u'information', u'israel', u'defense', u'force', u'homefront', u'command', u'dvir', u'reznik', u'advisor', u'startups', u'israel', u'told', u'bbc', u'yo', u'alerts', u'halfbaked', u'im', u'israel', u'missile', u'lands', u'miles', u'away', u'yo', u'warning', u'use', u'said', u'better', u'evolving', u'meaningful', u'needs', u'substance', u'make', u'convenient', u'useful', u'red', u'alert', u'retooling', u'yo', u'comes', u'time', u'increased', u'tension', u'israel', u'palestinians', u'similar', u'apps', u'gaza', u'palestinians', u'relying', u'instead', u'twitter', u'hashtags', u'avoid', u'danger', u'yo', u'recently', u'derided', u'critics', u'gimmick', u'accelerating', u'decline', u'humanity', u'yaniv', u'feldman', u'israelbased', u'tech', u'website', u'geektime', u'told', u'bbc', u'thought', u'yo', u'dumbest', u'ideas', u'beneficial', u'use', u'yo', u'red', u'alert', u'app', u'mashup', u'raise', u'awareness', u'people', u'israel', u'realise', u'missile', u'strikes', u'affect', u'anouk', u'lorie', u'editor', u'chief', u'nocamels', u'israeli', u'tech', u'blog', u'little', u'positive', u'red', u'alert', u'potentially', u'lifesaving', u'app', u'large', u'number', u'israelis', u'seconds', u'cover', u'dozens', u'daily', u'incoming', u'rockets', u'im', u'sure', u'yo', u'appropriate', u'word', u'pop', u'mobiles', u'said', u'hand', u'seeing', u'yo', u'screen', u'feels', u'distressing', u'loud', u'siren', u'default', u'red', u'alert', u'matter', u'pointless', u'people', u'deem', u'yo', u'app', u'just', u'slightly', u'lighten', u'difficult', u'situation', u'welcome', u'ari', u'sprung', u'coauthor', u'red', u'alert', u'told', u'bbc', u'mashup', u'definitely', u'giving', u'rest', u'world', u'sense', u'volume', u'missiles', u'launched', u'israel', u'hourly', u'basis', u'hopes', u'app', u'raise', u'awareness', u'bbc', u'responsible', u'content', u'external', u'internet', u'sites', u'years', u'negotiations', u'world', u'powers', u'reach', u'deal', u'iran', u'limiting', u'iranian', u'nuclear', u'activity', u'return', u'lifting', u'sanctions', u'man', u'bank', u'clerk', u'brilliant', u'spy', u'best', u'ways', u'fight', u'memory', u'loss', u'burt', u'bacharach', u'having', u'hits', u'reimagined', u'italian', u'ancestor', u'gave', u'syrian', u'family', u'ticket', u'safety', u'plant', u'thats', u'used', u'medicine', u'murder', u'weapon', u'leftwing', u'party', u'sits', u'mplus', u'property', u'asset', u'fighting', u'denial', u'stopping', u'end', u'ebola', u'sinister', u'roots', u'word', u'surveillance', u'famous', u'toy', u'shop', u'closes', u'new', u'york'])]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.select(\"filtered\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature hashing, TF-IDF\n",
    "\n",
    "\n",
    "### Feature hashing\n",
    "\n",
    "When doing \"bag of words\" type techniques on a *large* corpus and without an existing vocabulary, there is a simple trick that is often useful.  The issue (and solution) is as follows: \n",
    "\n",
    " - The output is a feature vector, so that whenever we encounter a word we must look up which coordinate slot it is in.  A naive way would be to keep a list of all the words encoutered so far, and look up each word when it is encountered.  Whenever we encounter a new word, we see if we've already seen it before and if not -- assign it a new number.  This requires storing all the words that we have seen in memory, cannot be done in parallel (because we'd have to share the hash-table of seen words), etc.\n",
    " - A **hash function** takes as input something complicated (like a string) and spits out a number, with the desired property being that different inputs *usually* produce different outputs.  (This is how hash tables are implemented, as the name suggests.)\n",
    " - So -- rather than exactly looking up the coordinate of a given word, we can just use its hash value (modulo a big size that we choose).  This is fast and parallelizes easily.  (There are some downsides: You cannot tell, after the fact, what word each of your feature actually corresponds to!)\n",
    " \n",
    " \n",
    "### TF-IDF weighting \n",
    "\n",
    "With single word vocabularies, we can probably do an okay job of coming up with a reasonable (if short) list of words that distinguish between the two documents.  With n-grams, even for $n=2$, it is better to let a computer help us.  \n",
    "\n",
    "We would like to find words that are common in one document, not not common in all of them.  This is the goal of the __td-idf weighting__.  A precise definition is:\n",
    "\n",
    "  1. If $d$ denotes a document and $t$ denotes a term, then the _raw term frequency_ $\\mathrm{tf}^{raw}(t,d)$ is\n",
    "  $$ \\mathrm{tf}^{raw}(t,d) = \\text{the number of times the term $t$ occurs in the document $d$} $$\n",
    "  The vector of all term frequencies can optionally be _normalized_ either by dividing by the maximum of ny single word's occurance count ($L^1$) or by the Euclidean length of the vector of word occurance counts ($L^2$).  Scikit-learn by defaults does this second one:\n",
    "  $$ \\mathrm{tf}(t,d) = \\mathrm{tf}^{L^2}(t,d) = \\frac{\\mathrm{tf}^{raw}(t,d)}{\\sqrt{\\sum_t \\mathrm{tf}^{raw}(t,d)^2}} $$\n",
    "  2. If $$ D = \\left\\{ d : d \\in D \\right\\} $$ is the set of possible documents, then  the _inverse document frequency_ is\n",
    "  $$ \\mathrm{idf}^{naive}(t,D) = \\log \\frac{\\# D}{\\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "  = \\log \\frac{\\text{count of all documents}}{\\text{count of those documents containing the term $t$}} $$\n",
    "  with a common variant being\n",
    "  $$ \\mathrm{idf}(t, D) = \\log \\frac{\\# D}{1 + \\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "   = \\log \\frac{\\text{count of all documents}}{1 + \\text{count of those documents containing the term $t$}} $$\n",
    "  (This second one is the default in scikit-learn. Without this tweak we would omit the $1+$ in the denominator and have to worry about dividing by zero if $t$ is not found in any documents.)\n",
    "  3. Finally, the weight that we assign to the term $t$ appearing in document $d$ and depending on the corpus of all documents $D$ is\n",
    "  $$ \\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\mathrm{idf}(t,D) $$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[3,19],[0.495...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[1,2,4,5,6,7,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "#Hashing\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurized_df = hashingTF.transform(filtered_df)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurized_df)\n",
    "rescaled_df = idfModel.transform(featurized_df)\n",
    "rescaled_df.select(\"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very common in practice to have sparse training data. Spark ML supports reading training examples stored in LIBSVM format. It is a text format in which each line represents a labeled sparse feature vector using the following format:\n",
    "\n",
    "label index1:value1 index2:value2 ...\n",
    "\n",
    "It has a type SparseVector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(20, {0: 1.0513, 1: 2.5628, 2: 4.3698, 3: 2.2317, 4: 1.8889, 5: 8.7509, 6: 1.9458, 7: 3.744, 8: 2.4062, 9: 2.944, 10: 3.1251, 11: 3.0351, 12: 0.9603, 13: 2.1758, 14: 2.5186, 15: 3.2417, 16: 2.6389, 17: 2.5166, 18: 1.6455, 19: 1.8112}))]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescaled_df.select(\"features\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Let us look at the two words: \"computer\" and \"computers\".  It would have been useful to identify them as one word.\n",
    "\n",
    "This is not limited to just trailing \"s\" characters: e.g., the words \"carry\", \"carries\", \"carrying\", and \"carried\" all carry -- roughly -- the same meaning.  The process of replacing them by a common root, or **stem**, is called stemming -- the stem will not, in general, be a full word itself.\n",
    "\n",
    "There's a related process called **lemmatization**: The analog of the \"stem\" here _is_ an actual word.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model\n",
    "\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "A decision tree is a binary tree.  At each of the internal nodes, it chooses a feature $i$ and a threshold $t$.  Each leaf has a value.  Evaluation of the model is just traversal of the tree from the root.  At each node, for example $j$, we go down the left branch if $X_{ji} \\le t$ and the right branch otherwise.  The value of the model $f(X_{ji})$ is the value at the value at the terminating leaf of this traveral.  Below, we show a picture of this on small decision tree trained on the iris data set.  Notice that each internal node has a decision criterion and each leaf has the breakdown of label classes left at this leaf of the tree.  For a geometric picture of a decision tree, take a look at this [blog post](https://shapeofdata.wordpress.com/2013/07/02/decision-trees/).\n",
    "\n",
    "\n",
    "### Decision Tree Training Algorithm and Tuning Parameters\n",
    "\n",
    "The algorithm to construct a Decision Tree recursively builds a tree structure.  At each node, it finds the split (the feature and threshold level) that maximize the improvement in a criteria (in this case, the decrease in the gini index.  This algorithm is controlled by four major parameters:\n",
    "\n",
    "<table>\n",
    "\t<tr>\n",
    "    <th>Feature</th>\n",
    "    <th>Value</th>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`max_features`</td>\n",
    "    <td>The number of features to consider when choosing a split for an internal node</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`max_depth`</td>\n",
    "    <td>The maximum depth of tree from the root</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`min_samples_split`</td>\n",
    "    <td>Minimum number of samples required for a split to be considered</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`min_samples_leaf`</td>\n",
    "    <td>Minimum number of samples required for each leaf</td>\n",
    "\t</tr>\n",
    "</table>\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "A random forest is just an ensemble of decision trees.  The predicted value is just the average of the trees (for both regression and classification problems - for classification problems, it is the probabilities that are averaged).  You can adjust `n_estimators` to change the number of trees in the forest.  If each tree is trained on the same subset of data, why aren't they identical?  Two reasons:\n",
    "1. **Subsampling**: each tree is actually trained on a random selected (with replacement) subset (i.e. bootstrap)\n",
    "1. **Maximum Features**: the optimal split comes from a randomly selected subset of the features.  In scikit-learn, this feature is controlled by `max_features`.\n",
    "\n",
    "\n",
    "## Extremely Random Forests\n",
    "Instead of choosing the optimal split amongst a (randomly selected) subset of features, we choose random values we choose amongst randomly generated thresholds.  While the first two are options in scikit, this is implemented in `ExtraTreesClassifier`.\n",
    "\n",
    "**Question**: What happens to bias and variance of the individual trees in the averaging process of Random Forests and Extremely Random Forests.  How would you change your parameters to compensate?\n",
    "\n",
    "You can read more about these [here](http://scikit-learn.org/0.12/modules/ensemble.html).\n",
    "\n",
    "## Random Forest Training Algorithm and Tuning Parameters\n",
    "\n",
    "A Random Forest and Extremely Random Forest are pretty straightforward to train once you know how a Decision Tree works.  In fact, their construction can even be parallelized.  They have an extra parameter `n_estimators` and their construction can be parallelized by setting the parameter `n_jobs`.\n",
    "\n",
    "\n",
    "### Decision Trees\n",
    "1. Increasing `max_features` and `max_depth` and decreasing `min_samples_split` and `min_samples_leaf` tend to build more complex models (increase Variance and reduce Bias).\n",
    "1. Straightfoward.\n",
    "\n",
    "### Random Forests\n",
    "The variance between different trees tends to cancel each other while the biases reinforce each other.  That is, becasue the trees are different, they tend to overfit in different ways but when they underfit, they underfit the same way.  So you want to use higher variance, lower bias parameters than you would with a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=2)\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\",numTrees=10,impurity=\"gini\",maxDepth=4,maxBins=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all into a machine learning pipeline\n",
    "\n",
    "In machine learning, it is common to run a sequence of algorithms to process and learn from data. E.g., a simple text document processing workflow might include several stages:\n",
    "\n",
    "<img src=\"http://spark.apache.org/docs/latest/img/ml-PipelineModel.png\">\n",
    "\n",
    "\n",
    "## How it works\n",
    "A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator (see slides for the meaning). These stages are run in order, and the input DataFrame is transformed as it passes through each stage. For Transformer stages, the transform() method is called on the DataFrame. For Estimator stages, the fit() method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline), and that Transformers transform() method is called on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, labelIndexer, featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a model\n",
    "\n",
    "## Metrics for Classificaton\n",
    "\n",
    "There are a plethora of metrics for classification and they depend on whether the predictions are given in terms of the potential label classes or probabilities.\n",
    "\n",
    "### Metrics for Class Predictions\n",
    "\n",
    "Let's start with the simplest.\n",
    "\n",
    "Recall this well-known table\n",
    "\n",
    "|                     | Observation Positive     | Observation Negative    |\n",
    "|---------------------|:------------------------:|:-----------------------:|\n",
    "| Prediction Positive |     True Positive        | False Positive (Type I) |\n",
    "| Prediction Negative | False Negative (Type II) |     True Negative       |\n",
    "\n",
    "There are many summary statistics one can compute from this table:\n",
    "1. The **Accuracy** gives the fraction labels correctly predicted (True Positives and True Negatives over everything).  \n",
    "1. The **Hamming Loss** gives the fraction of labels incorrectly predicted.  It is 1 - Accuracy.\n",
    "1. The **Precision** is true positives divided by all positive predictions \n",
    "1. The **Recall** is true positives divided by all positive observations.\n",
    "1. There is also **F-beta** score which gives a weighted geometric average between the precision and recall (as a function of $\\beta$) and the **F-1** score is the special case when $\\beta = 1$.\n",
    "1. The **Jaccard Similarity Coefficient** is the True positives divided by the sum of true positives, false negatives, and false positives.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named sklearn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-22e62483a164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Accuracy and Hamming distnace:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_obs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named sklearn"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Accuracy and Hamming distnace:\n",
    "\n",
    "y_obs  = [0, 0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [0, 0, 1, 1, 0, 0, 0, 1]\n",
    "\n",
    "print \"Accuracy:\", metrics.accuracy_score(y_obs, y_pred)\n",
    "print \"Hamming Loss:\", metrics.hamming_loss(y_obs, y_pred)\n",
    "print \"Precision:\", metrics.precision_score(y_obs, y_pred)\n",
    "print \"Recall:\", metrics.recall_score(y_obs, y_pred)\n",
    "print \"F1:\", metrics.f1_score(y_obs, y_pred)\n",
    "print \"Jaccard:\", metrics.jaccard_similarity_score(y_obs, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary and multiclass classification evaluators in spark ml\n",
    "\n",
    "In Spark ML, we have two main classes for evaluators:\n",
    "\n",
    "**BinaryClassificationEvaluator**: evaluator for binary classification, which expects two input columns: score and label.\n",
    "It uses areaUnderROC (area under the receiver operating characteristic curve) and areaUnderPR (area under the precision-recall curve) metrics to evaluate a classifier. \n",
    "\n",
    "**MulticlassClassificationEvaluator**: evaluator for multi-class classification.\n",
    "metricName options are f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate model on test instances and compute test error...\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|label|                text|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  0.0|insolite art et a...|[0.83245651855739...|       0.0|\n",
      "|  0.0| interrobang rest...|[0.80869206280577...|       0.0|\n",
      "|  0.0|                    |[0.88126860729414...|       0.0|\n",
      "|  0.0|meet whitney krop...|[0.76886232863934...|       0.0|\n",
      "|  0.0|we may not have h...|[0.79327208645344...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "areaUnderPR :  0.213388487838\n",
      "CV Error = 0.816860465116\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "#Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "#is areaUnderROC.\n",
    "#metricName options are: areaUnderROC|areaUnderPR)\n",
    "metricName = \"areaUnderPR\"\n",
    "ev = BinaryClassificationEvaluator(metricName=metricName)\n",
    "#Alternative: user multiclass classification evaluator\n",
    "#metricName options are f1, precision, recall\n",
    "#ev = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "print \"Evaluate model on test instances and compute test error...\"\n",
    "prediction = model.transform(cv)\n",
    "#prediction = labelConverter.transform(prediction)\n",
    "prediction.select(\"label\", \"text\", \"probability\", \"prediction\").show(5)\n",
    "\n",
    "result = ev.evaluate(prediction)\n",
    "print metricName,\": \", result\n",
    "\n",
    "cvErr = prediction.filter(prediction.label == prediction.prediction).count() / float(cv.count())\n",
    "print 'CV Error = ' + str(cvErr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter search\n",
    "\n",
    "How can we further improve the perfromance? We can perfrom a hyper parameter search to make sure we get the most of our classifier.\n",
    "\n",
    "We are going to use the CrossValidator to select from a grid of parameters. To help construct the parameter grid, we will use the ParamGridBuilder utility. At the moment of preparing this materials, only the Scala implementation was fully working, so we are going to switch to the Scala-based implementation till the rest of the night and follow it as a demo-only:\n",
    "https://github.com/ASvyatkovskiy/MLmeetup2016/blob/master/classification/src/main/scala/ClassificationAdhoc.scala\n",
    "\n",
    "Note that cross-validation over a grid of parameters is expensive. However, it is also a well-established method for choosing parameters which is more statistically sound than heuristic hand-tuning.\n",
    "\n",
    "Here is what we are going to start with:\n",
    "\n",
    "\n",
    "```scala\n",
    "    var paramGrid = new ParamGridBuilder()\n",
    "                    .addGrid(hashingTF.numFeatures, Array(10, 20, 100))\n",
    "                    .addGrid(rf.numTrees, Array(3, 5, 10))\n",
    "                    .build()\n",
    "  \n",
    "    //set estimator \n",
    "    var crossval = new CrossValidator().setEstimator(rf).\n",
    "                              setEstimatorParamMaps(paramGrid).\n",
    "                              setEvaluator(ev).\n",
    "                              setNumFolds(3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding custom features to the model\n",
    "\n",
    "So far we were dealing with text features only, and we were able to achieve 95% accuracy on the cross validation set.\n",
    "As you know, the structure of our data is as follows:\n",
    "\n",
    "```bash\n",
    "{\"namespace\": \"html.avro\",\n",
    " \"type\": \"record\",\n",
    " \"name\": \"Html\",\n",
    " \"fields\": [\n",
    "     {\"name\": \"id\", \"type\": \"string\"},\n",
    "     {\"name\": \"images\",  \"type\": {\"type\":\"array\", \"items\":\"string\"}},\n",
    "     {\"name\": \"links\", \"type\": {\"type\":\"array\", \"items\":\"string\"}},\n",
    "     {\"name\": \"text\", \"type\": \"string\"},\n",
    "     {\"name\": \"title\", \"type\": {\"type\":\"array\", \"items\":\"string\"}}\n",
    " ]\n",
    "}\n",
    "```\n",
    "\n",
    "So we have the info about the page title, links, and images. The simpels adhoc features one can construct and test are the count features. Namely, the number of links on a page, number of images.\n",
    "\n",
    "There is no built-in functionality in Spark ML which would allow to modify the **features** column of the dataframe by appending two more components to the SparseVector which is the underlying container there.\n",
    "\n",
    "So a simple starting point can be this UDF:\n",
    "\n",
    "```scala\n",
    "  def appendFeature(sv: SparseVector, adhoc_feature1: Int, adhoc_feature2: Int) : Vector = {\n",
    "\n",
    "      val inputValues = sv.values\n",
    "      val inputIndices = sv.indices\n",
    "      val inputValuesLength = inputValues.length\n",
    "      val dim = sv.size\n",
    "\n",
    "      var adhoc_features = Array(adhoc_feature1,adhoc_feature2)\n",
    "      val addhoc_size = adhoc_features.length\n",
    "\n",
    "      val outputValues = Array.ofDim[Double](inputValuesLength + addhoc_size)\n",
    "      val outputIndices = Array.ofDim[Int](inputValuesLength + addhoc_size)\n",
    "\n",
    "      System.arraycopy(inputValues, 0, outputValues, 0, inputValuesLength)\n",
    "      System.arraycopy(inputIndices, 0, outputIndices, 0, inputValuesLength)\n",
    "\n",
    "      for (i <- 1 to addhoc_size) {\n",
    "        outputValues(inputValuesLength-1+i) = adhoc_features(i-1).toDouble\n",
    "      }\n",
    "      outputIndices(inputValuesLength) = dim\n",
    "\n",
    "      Vectors.sparse(dim + addhoc_size, outputIndices, outputValues)\n",
    "\n",
    "  }\n",
    "```  \n",
    "\n",
    "Which can be applied in Spark like this:\n",
    "\n",
    "```scala\n",
    "    def appendFeature_udf = udf(appendFeature _)\n",
    "    var adhoc_df = rescaled_df.withColumn(\"features\", appendFeature_udf(col(\"pre_features\"),col(\"links_cnt\"),col(\"images_cnt\"))) \n",
    "```    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
